[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Downloading the Dataset and the weights of the pre trained model",
    "section": "",
    "text": "Packages Installation\n\n!pip install -q rfdetr==1.2.1 supervision==0.26.1 roboflow\n!pip install paddleocr\n!pip install pymupdf pillow\n!pip install paddlepaddle\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/266.3 kB ? eta -:--:--     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 266.3/266.3 kB 16.2 MB/s eta 0:00:00\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Installing backend dependencies ... done\n  Preparing metadata (pyproject.toml) ... done\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 131.3/131.3 kB 11.6 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.2/207.2 kB 21.4 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.9/89.9 kB 8.8 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.8/66.8 kB 4.1 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.9/49.9 MB 21.0 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 64.5 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/4.2 MB 90.2 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 63.0 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 372.8/372.8 kB 21.1 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.8/44.8 kB 3.9 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 70.3 MB/s eta 0:00:00\n  Building wheel for fairscale (pyproject.toml) ... done\nCollecting paddleocr\n  Downloading paddleocr-3.3.2-py3-none-any.whl.metadata (55 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.2/55.2 kB 6.2 MB/s eta 0:00:00\nCollecting paddlex&lt;3.4.0,&gt;=3.3.0 (from paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr)\n  Downloading paddlex-3.3.11-py3-none-any.whl.metadata (79 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.9/79.9 kB 9.7 MB/s eta 0:00:00\nRequirement already satisfied: PyYAML&gt;=6 in /usr/local/lib/python3.12/dist-packages (from paddleocr) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from paddleocr) (2.32.4)\nRequirement already satisfied: typing-extensions&gt;=4.12 in /usr/local/lib/python3.12/dist-packages (from paddleocr) (4.15.0)\nCollecting aistudio-sdk&gt;=0.3.5 (from paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr)\n  Downloading aistudio_sdk-0.3.8-py3-none-any.whl.metadata (1.1 kB)\nRequirement already satisfied: chardet in /usr/local/lib/python3.12/dist-packages (from paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (5.2.0)\nCollecting colorlog (from paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr)\n  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (3.20.0)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (0.36.0)\nCollecting modelscope&gt;=1.28.0 (from paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr)\n  Downloading modelscope-1.33.0-py3-none-any.whl.metadata (43 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.3/43.3 kB 4.9 MB/s eta 0:00:00\nRequirement already satisfied: numpy&gt;=1.24 in /usr/local/lib/python3.12/dist-packages (from paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (2.0.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (25.0)\nRequirement already satisfied: pandas&gt;=1.3 in /usr/local/lib/python3.12/dist-packages (from paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (2.2.2)\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (11.3.0)\nRequirement already satisfied: prettytable in /usr/local/lib/python3.12/dist-packages (from paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (3.17.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (9.0.0)\nRequirement already satisfied: pydantic&gt;=2 in /usr/local/lib/python3.12/dist-packages (from paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (2.12.3)\nCollecting PyYAML&gt;=6 (from paddleocr)\n  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\nCollecting ruamel.yaml (from paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr)\n  Downloading ruamel.yaml-0.18.16-py3-none-any.whl.metadata (25 kB)\nCollecting ujson (from paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr)\n  Downloading ujson-5.11.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (9.4 kB)\nRequirement already satisfied: imagesize in /usr/local/lib/python3.12/dist-packages (from paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (1.4.1)\nCollecting opencv-contrib-python==4.10.0.84 (from paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr)\n  Downloading opencv_contrib_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nCollecting pyclipper (from paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr)\n  Downloading pyclipper-1.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (8.6 kB)\nCollecting pypdfium2&gt;=4 (from paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr)\n  Downloading pypdfium2-5.1.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.7/67.7 kB 6.6 MB/s eta 0:00:00\nCollecting python-bidi (from paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr)\n  Downloading python_bidi-0.6.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: shapely in /usr/local/lib/python3.12/dist-packages (from paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (2.1.2)\nRequirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;paddleocr) (3.4.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;paddleocr) (3.7)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;paddleocr) (2.5.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests-&gt;paddleocr) (2025.11.12)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from aistudio-sdk&gt;=0.3.5-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (5.9.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from aistudio-sdk&gt;=0.3.5-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (4.67.1)\nCollecting bce-python-sdk (from aistudio-sdk&gt;=0.3.5-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr)\n  Downloading bce_python_sdk-0.9.55-py3-none-any.whl.metadata (416 bytes)\nRequirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from aistudio-sdk&gt;=0.3.5-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (8.3.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from modelscope&gt;=1.28.0-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (75.2.0)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas&gt;=1.3-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas&gt;=1.3-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas&gt;=1.3-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (2025.2)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic&gt;=2-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic&gt;=2-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (2.41.4)\nRequirement already satisfied: typing-inspection&gt;=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic&gt;=2-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (0.4.2)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (2025.3.0)\nRequirement already satisfied: hf-xet&lt;2.0.0,&gt;=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (1.2.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prettytable-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (0.2.14)\nCollecting ruamel.yaml.clib&gt;=0.2.7 (from ruamel.yaml-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr)\n  Downloading ruamel_yaml_clib-0.2.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=1.3-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (1.17.0)\nCollecting pycryptodome&gt;=3.8.0 (from bce-python-sdk-&gt;aistudio-sdk&gt;=0.3.5-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr)\n  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\nRequirement already satisfied: future&gt;=0.6.0 in /usr/local/lib/python3.12/dist-packages (from bce-python-sdk-&gt;aistudio-sdk&gt;=0.3.5-&gt;paddlex&lt;3.4.0,&gt;=3.3.0-&gt;paddlex[ocr-core]&lt;3.4.0,&gt;=3.3.0-&gt;paddleocr) (1.0.0)\nDownloading paddleocr-3.3.2-py3-none-any.whl (86 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.0/87.0 kB 10.0 MB/s eta 0:00:00\nDownloading paddlex-3.3.11-py3-none-any.whl (1.9 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 86.1 MB/s eta 0:00:00\nDownloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 767.5/767.5 kB 46.3 MB/s eta 0:00:00\nDownloading opencv_contrib_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (68.7 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 68.7/68.7 MB 10.9 MB/s eta 0:00:00\nDownloading aistudio_sdk-0.3.8-py3-none-any.whl (62 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.0/63.0 kB 6.8 MB/s eta 0:00:00\nDownloading modelscope-1.33.0-py3-none-any.whl (6.1 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.1/6.1 MB 101.5 MB/s eta 0:00:00\nDownloading pypdfium2-5.1.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 75.3 MB/s eta 0:00:00\nDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\nDownloading pyclipper-1.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (978 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 978.2/978.2 kB 75.8 MB/s eta 0:00:00\nDownloading python_bidi-0.6.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 300.6/300.6 kB 33.9 MB/s eta 0:00:00\nDownloading ruamel.yaml-0.18.16-py3-none-any.whl (119 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.9/119.9 kB 14.0 MB/s eta 0:00:00\nDownloading ujson-5.11.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (57 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.4/57.4 kB 6.7 MB/s eta 0:00:00\nDownloading ruamel_yaml_clib-0.2.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (788 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 788.2/788.2 kB 57.6 MB/s eta 0:00:00\nDownloading bce_python_sdk-0.9.55-py3-none-any.whl (390 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 390.5/390.5 kB 38.2 MB/s eta 0:00:00\nDownloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 110.7 MB/s eta 0:00:00\nInstalling collected packages: python-bidi, ujson, ruamel.yaml.clib, PyYAML, pypdfium2, pycryptodome, pyclipper, opencv-contrib-python, colorlog, ruamel.yaml, modelscope, bce-python-sdk, aistudio-sdk, paddlex, paddleocr\n  Attempting uninstall: PyYAML\n    Found existing installation: PyYAML 6.0.3\n    Uninstalling PyYAML-6.0.3:\n      Successfully uninstalled PyYAML-6.0.3\n  Attempting uninstall: opencv-contrib-python\n    Found existing installation: opencv-contrib-python 4.12.0.88\n    Uninstalling opencv-contrib-python-4.12.0.88:\n      Successfully uninstalled opencv-contrib-python-4.12.0.88\nSuccessfully installed PyYAML-6.0.2 aistudio-sdk-0.3.8 bce-python-sdk-0.9.55 colorlog-6.10.1 modelscope-1.33.0 opencv-contrib-python-4.10.0.84 paddleocr-3.3.2 paddlex-3.3.11 pyclipper-1.4.0 pycryptodome-3.23.0 pypdfium2-5.1.0 python-bidi-0.6.7 ruamel.yaml-0.18.16 ruamel.yaml.clib-0.2.15 ujson-5.11.0\nCollecting pymupdf\n  Downloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\nDownloading pymupdf-1.26.6-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.1/24.1 MB 29.2 MB/s eta 0:00:00\nInstalling collected packages: pymupdf\nSuccessfully installed pymupdf-1.26.6\nCollecting paddlepaddle\n  Downloading paddlepaddle-3.2.2-cp312-cp312-manylinux1_x86_64.whl.metadata (8.8 kB)\nRequirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from paddlepaddle) (0.28.1)\nRequirement already satisfied: numpy&gt;=1.21 in /usr/local/lib/python3.12/dist-packages (from paddlepaddle) (2.0.2)\nRequirement already satisfied: protobuf&gt;=3.20.2 in /usr/local/lib/python3.12/dist-packages (from paddlepaddle) (5.29.5)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from paddlepaddle) (11.3.0)\nCollecting opt_einsum==3.3.0 (from paddlepaddle)\n  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from paddlepaddle) (3.6)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from paddlepaddle) (4.15.0)\nRequirement already satisfied: safetensors&gt;=0.6.0 in /usr/local/lib/python3.12/dist-packages (from paddlepaddle) (0.7.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx-&gt;paddlepaddle) (4.12.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx-&gt;paddlepaddle) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx-&gt;paddlepaddle) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx-&gt;paddlepaddle) (3.7)\nRequirement already satisfied: h11&gt;=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*-&gt;httpx-&gt;paddlepaddle) (0.16.0)\nDownloading paddlepaddle-3.2.2-cp312-cp312-manylinux1_x86_64.whl (189.3 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 189.3/189.3 MB 6.5 MB/s eta 0:00:00\nDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 6.9 MB/s eta 0:00:00\nInstalling collected packages: opt_einsum, paddlepaddle\n  Attempting uninstall: opt_einsum\n    Found existing installation: opt_einsum 3.4.0\n    Uninstalling opt_einsum-3.4.0:\n      Successfully uninstalled opt_einsum-3.4.0\nSuccessfully installed opt_einsum-3.3.0 paddlepaddle-3.2.2\n\n\n\n!pip install gdown\n\nRequirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\nRequirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\nRequirement already satisfied: soupsieve&gt;1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4-&gt;gdown) (2.8)\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4-&gt;gdown) (4.15.0)\nRequirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]-&gt;gdown) (3.4.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]-&gt;gdown) (3.7)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]-&gt;gdown) (2.5.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]-&gt;gdown) (2025.11.12)\nRequirement already satisfied: PySocks!=1.5.7,&gt;=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]-&gt;gdown) (1.7.1)\n\n\nDownload the Dataset\n\n!pip install --quiet gdown\n\n\nimport re, subprocess, shutil, os\n\ndef download_and_extract(link_or_id, dest=\"Dataset\"):\n    m = re.search(r'id=([A-Za-z0-9_-]+)', link_or_id) or re.search(r'/d/([A-Za-z0-9_-]+)', link_or_id)\n    if not m:\n        raise ValueError(\"Paste a valid Google Drive file link\")\n    fid = m.group(1)\n\n    print(\"Downloading zip...\")\n    url = f\"https://drive.google.com/uc?id={fid}\"\n    subprocess.run([\"gdown\", url, \"-O\", \"dataset.zip\"], check=True)\n\n    if os.path.exists(dest):\n        shutil.rmtree(dest)\n\n    os.makedirs(dest, exist_ok=True)\n\n    print(\"Extracting zip to folder...\")\n    subprocess.run([\"unzip\", \"-q\", \"dataset.zip\", \"-d\", dest], check=True)\n\n    print(f\"Done — check the local '{dest}' folder.\")\n    \nlink = \"https://drive.google.com/file/d/147pOV428WE6wdpvcL8uyNKm96LgBj3VF/view?usp=sharing\"\ndownload_and_extract(link, dest=\"Dataset\")\n\nDownloading zip...\nExtracting zip to folder...\nDone — check the local 'Dataset' folder.\n\n\nDownload the Weights of the RF-DETR\n\n!pip install --quiet gdown\n\n\nimport re\nimport subprocess\nfrom pathlib import Path\n\ndef download_drive_to_weights(link_or_id):\n    dest = \"weights\"\n    Path(dest).mkdir(parents=True, exist_ok=True)\n    print(f\"Using '{dest}' folder for downloads...\")\n\n    s = str(link_or_id).strip()\n\n    m = re.search(r'folders/([A-Za-z0-9_-]+)', s) or re.search(r'id=([A-Za-z0-9_-]+)', s)\n    if not m:\n        m = re.search(r'/d/([A-Za-z0-9_-]+)', s) or re.search(r'([A-Za-z0-9_-]{10,})$', s)\n    if not m:\n        print(\"Couldn't parse an ID from the input. Please paste the folder link or the long '1...' id.\")\n        return\n    fid = m.group(1)\n\n    use_folder = ('folders' in s) or (len(fid) &gt; 20)\n\n    try:\n        if use_folder:\n            print(\"Downloading Drive folder to ./weights ...\")\n            subprocess.run(\n                [\"gdown\", \"--folder\", f\"https://drive.google.com/drive/folders/{fid}\", \"-O\", dest],\n                check=True\n            )\n        else:\n            print(\"Downloading file to ./weights ...\")\n            url = f\"https://drive.google.com/uc?id={fid}\"\n            subprocess.run(\n                [\"gdown\", url, \"-O\", dest],\n                check=True\n            )\n\n        print(\"Done — check the local 'weights' folder.\")\n    except subprocess.CalledProcessError as e:\n        print(\"gdown failed:\", e)\n        \n    link_or_id = \"https://drive.google.com/drive/folders/17CXXFCGw5npR1cyrg082yq1IdcnH6-ms?usp=sharing\"\ndownload_drive_to_weights(link_or_id)\n\nUsing 'weights' folder for downloads...\nDownloading Drive folder to ./weights ...\nDone — check the local 'weights' folder.\n\n\nClone the Sam2 Model\n\n!git clone https://github.com/facebookresearch/sam2.git\n\n!pip install -e .\n\nCloning into 'sam2'...\nremote: Enumerating objects: 1070, done.\nremote: Total 1070 (delta 0), reused 0 (delta 0), pack-reused 1070 (from 1)\nReceiving objects: 100% (1070/1070), 128.11 MiB | 11.61 MiB/s, done.\nResolving deltas: 100% (380/380), done.\nUpdating files: 100% (569/569), done.\n/content/sam2\nObtaining file:///content/sam2\n  Installing build dependencies ... done\n  Checking if build backend supports build_editable ... done\n  Getting requirements to build editable ... done\n  Preparing editable metadata (pyproject.toml) ... done\nRequirement already satisfied: torch&gt;=2.5.1 in /usr/local/lib/python3.12/dist-packages (from SAM-2==1.0) (2.9.0+cu126)\nRequirement already satisfied: torchvision&gt;=0.20.1 in /usr/local/lib/python3.12/dist-packages (from SAM-2==1.0) (0.24.0+cu126)\nRequirement already satisfied: numpy&gt;=1.24.4 in /usr/local/lib/python3.12/dist-packages (from SAM-2==1.0) (2.0.2)\nRequirement already satisfied: tqdm&gt;=4.66.1 in /usr/local/lib/python3.12/dist-packages (from SAM-2==1.0) (4.67.1)\nCollecting hydra-core&gt;=1.3.2 (from SAM-2==1.0)\n  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nCollecting iopath&gt;=0.1.10 (from SAM-2==1.0)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.2/42.2 kB 4.6 MB/s eta 0:00:00\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: pillow&gt;=9.4.0 in /usr/local/lib/python3.12/dist-packages (from SAM-2==1.0) (11.3.0)\nRequirement already satisfied: omegaconf&lt;2.4,&gt;=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core&gt;=1.3.2-&gt;SAM-2==1.0) (2.3.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core&gt;=1.3.2-&gt;SAM-2==1.0) (4.9.3)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core&gt;=1.3.2-&gt;SAM-2==1.0) (25.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath&gt;=0.1.10-&gt;SAM-2==1.0) (4.15.0)\nCollecting portalocker (from iopath&gt;=0.1.10-&gt;SAM-2==1.0)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (3.20.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (75.2.0)\nRequirement already satisfied: sympy&gt;=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (1.14.0)\nRequirement already satisfied: networkx&gt;=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (3.6)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (3.1.6)\nRequirement already satisfied: fsspec&gt;=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (2.27.5)\nRequirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (3.3.20)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (1.11.1.6)\nRequirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch&gt;=2.5.1-&gt;SAM-2==1.0) (3.5.0)\nRequirement already satisfied: PyYAML&gt;=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf&lt;2.4,&gt;=2.2-&gt;hydra-core&gt;=1.3.2-&gt;SAM-2==1.0) (6.0.2)\nRequirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy&gt;=1.13.3-&gt;torch&gt;=2.5.1-&gt;SAM-2==1.0) (1.3.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2-&gt;torch&gt;=2.5.1-&gt;SAM-2==1.0) (3.0.3)\nDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.5/154.5 kB 15.6 MB/s eta 0:00:00\nDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nBuilding wheels for collected packages: SAM-2, iopath\n  Building editable for SAM-2 (pyproject.toml) ... done\n  Created wheel for SAM-2: filename=sam_2-1.0-0.editable-cp312-cp312-linux_x86_64.whl size=13852 sha256=f87822b85d374b5db5dd1b43bd7b95cd61395b461bf2e45266cae9a65f542ff4\n  Stored in directory: /tmp/pip-ephem-wheel-cache-hd59s1yu/wheels/9e/fa/17/14aaeb20d3ca07c58ee93742054d4479f89c243063ce0b61b9\n  Building wheel for iopath (setup.py) ... done\n  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=650d2af9f6e79b48459227f4fe01035aac6dcc406f998403e66fe84cc26aabee\n  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\nSuccessfully built SAM-2 iopath\nInstalling collected packages: portalocker, iopath, hydra-core, SAM-2\nSuccessfully installed SAM-2-1.0 hydra-core-1.3.2 iopath-0.1.10 portalocker-3.2.0\n\n\n\n\n\n/content\n\n\nDownlaod the weights of angle models\n\n!pip install --quiet gdown\n\nimport re\nimport shutil\nimport os\nimport subprocess\nimport zipfile\n\ndef download_and_extract_strip_top_folder(link_or_id, dest=\"/content/angle-models-weights\"):\n    m = re.search(r'id=([A-Za-z0-9_-]+)', link_or_id) or re.search(r'/d/([A-Za-z0-9_-]+)', link_or_id)\n    if not m:\n        raise ValueError(\"Paste a valid Google Drive file link\")\n    fid = m.group(1)\n\n    print(\"Downloading zip...\")\n    url = f\"https://drive.google.com/uc?id={fid}\"\n\n    if os.path.exists(dest):\n        shutil.rmtree(dest)\n    os.makedirs(dest)\n\n    subprocess.run([\"gdown\", url, \"-O\", \"weights.zip\"], check=True)\n\n    print(\"Extracting zip file and stripping top-level folder...\")\n\n    with zipfile.ZipFile(\"weights.zip\", 'r') as zip_ref:\n\n        all_paths = zip_ref.namelist()\n        top_level_dirs = set([p.split('/')[0] for p in all_paths if p.strip() != ''])\n        if len(top_level_dirs) == 1:\n            top_folder = list(top_level_dirs)[0]\n        else:\n            top_folder = None\n\n        for member in all_paths:\n            if top_folder:\n                rel_path = member[len(top_folder)+1:] if member.startswith(top_folder + '/') else member\n            else:\n                rel_path = member\n\n            if rel_path == '':\n\n                continue\n\n            dest_path = os.path.join(dest, rel_path)\n            if member.endswith('/'):\n                os.makedirs(dest_path, exist_ok=True)\n            else:\n                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n                with zip_ref.open(member) as source, open(dest_path, 'wb') as target:\n                    shutil.copyfileobj(source, target)\n\n    print(f\"Done — check the local '{dest}' folder.\")\n\n\nlink = \"https://drive.google.com/file/d/1uI2GlE5JviTJccZxzLP5lQ6YiwjYTvVe/view?usp=sharing\"\ndownload_and_extract_strip_top_folder(link, dest=\"/content/angle-models-weights\")\n\nDownloading zip...\nExtracting zip file and stripping top-level folder...\nDone — check the local '/content/angle-models-weights' folder.\n\n\nDownload the weights of sam2\n\nimport re\nimport os\nimport subprocess\n\ndef download_file_from_gdrive(link_or_id, dest_folder=\"/content/sam2/checkpoints\", filename=None):\n    m = re.search(r'id=([A-Za-z0-9_-]+)', link_or_id) or re.search(r'/d/([A-Za-z0-9_-]+)', link_or_id)\n    if not m:\n        raise ValueError(\"Paste a valid Google Drive file link\")\n    fid = m.group(1)\n\n    if not os.path.exists(dest_folder):\n        os.makedirs(dest_folder)\n\n    url = f\"https://drive.google.com/uc?id={fid}\"\n\n    if not filename:\n\n        filename = f\"{fid}.pt\"\n\n    file_path = os.path.join(dest_folder, filename)\n\n    print(f\"Downloading file to {file_path} ...\")\n    subprocess.run([\"gdown\", url, \"-O\", file_path], check=True)\n    print(\"Download complete.\")\n\n\nlink = \"https://drive.google.com/file/d/11L0iHZ1Ktcjhd0vqKI_3-PJ3DTv2UyOc/view?usp=sharing\"\n\ndownload_file_from_gdrive(link, dest_folder=\"/content/sam2/checkpoints\", filename=\"sam2_hiera_base_plus.pt\")\n\nDownloading file to /content/sam2/checkpoints/sam2_hiera_base_plus.pt ...\nDownload complete.\n\n\n\n\nRF-DETR Model for shape and arrow detection\n\ndataset='/content/Dataset/AiBoardScannerDataSet.coco'\n\nMetric evaluation and ploting\n\nfrom PIL import Image\n\nImage.open(\"/content/weights/metrics_plot.png\")\n\n\n\n\n\n\n\n\npath of the pre-train weights and path of the image to to check the shape detection\n\npreTrainedWeights=\"/content/weights/checkpoint_best_regular.pth\"\n\n\nimport fitz\nfrom PIL import Image\nimport io\nimport os\n\nfile_path = \"/content/Ai-diagrams-board-25-_jpg.rf.47ff3ffa292e044b3822c0c83022ee77.jpg\"\noutput_image_path = \"/content/converted_image.png\"\n\nimgPath = None\n\nif not os.path.exists(file_path):\n    print(f\"Error: File not found at {file_path}\")\nelse:\n    if file_path.lower().endswith('.pdf'):\n        try:\n            doc = fitz.open(file_path)\n            page = doc.load_page(0)\n            pix = page.get_pixmap()\n            pix.save(output_image_path)\n            imgPath = Image.open(output_image_path).convert(\"RGB\")\n            print(\"PDF converted to image and saved successfully.\")\n            doc.close()\n        except Exception as e:\n            print(f\"Error converting PDF: {e}\")\n    else:\n        try:\n            imgPath = Image.open(file_path).convert(\"RGB\")\n            print(\"Image loaded successfully.\")\n        except Exception as e:\n            print(f\"Error opening image: {e}\")\n\nImage loaded successfully.\n\n\n\nimport io\nimport requests\nimport supervision as sv\nfrom PIL import Image\nfrom rfdetr import RFDETRMedium\nfrom rfdetr.util.coco_classes import COCO_CLASSES\n\nmodel = RFDETRMedium(pretrain_weights=preTrainedWeights)\nmodel.optimize_for_inference()\n\nimage = imgPath\n\ndetections = model.predict(image, threshold=0.60)\n\ntext_scale = sv.calculate_optimal_text_scale(image.size)\nthickness = sv.calculate_optimal_line_thickness(image.size)\ncolor = sv.ColorPalette.from_hex([\n    \"#FFFF00\", \"#FF9B00\", \"#FF66FF\", \"#3399FF\", \"#FF66B2\", \"#FF8080\",\n    \"#B266FF\", \"#9999FF\", \"#66FFFF\", \"#33FF99\", \"#66FF66\", \"#99FF00\"\n])\n\nbbox_annotator = sv.BoxAnnotator(color=color, thickness=thickness)\nlabel_annotator = sv.LabelAnnotator(color=color, text_color=sv.Color.BLACK, text_scale=text_scale)\nmy_classes = [\n    \"objects\",\n    \"Cloud\",\n    \"Diamond\",\n    \"Double Arrow\",\n    \"Pentagon\",\n    \"Racetrack\",\n    \"Star\",\n    \"Sticky Notes\",\n    \"Triangle\",\n    \"arrow\",\n    \"arrow_head\",\n    \"circle\",\n    \"dashed-arrow\",\n    \"dotted-arrow\",\n    \"rectangle\",\n    \"rounded rectangle\",\n    \"solid-arrow\"\n]\ndetection_labels = [\n    f\"{my_classes[class_id]} {conf:.2f}\"\n    for class_id, conf in zip(detections.class_id, detections.confidence)\n]\n\ndetections_list=[]\nfor bbox, class_id, conf in zip(detections.xyxy, detections.class_id, detections.confidence):\n    x_min, y_min, x_max, y_max = bbox\n    class_name = my_classes[class_id]\n    detection_tuple = (class_name, (float(x_min), float(y_min)), (float(x_max), float(y_max)))\n    detections_list.append(detection_tuple)\nprint(\"detections = [\")\nfor d in detections_list:\n    print(f\"    {d},\")\nprint(\"]\")\n\nannotated_image = image.copy()\nannotated_image = bbox_annotator.annotate(annotated_image, detections)\nannotated_image = label_annotator.annotate(annotated_image, detections, detection_labels)\n\nsv.plot_image(annotated_image)\n\nUserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n\n\nUsing a different number of positional encodings than DINOv2, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.\nUsing patch size 16 instead of 14, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.\nLoading pretrain weights\n\n\nWARNING:rfdetr.main:num_classes mismatch: pretrain weights has 16 classes, but your model has 90 classes\nreinitializing detection head with 16 classes\n`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\nTracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\nTracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\nTracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\nUserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4317.)\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\nTracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n\n\ndetections = [\n    ('Racetrack', (525.43798828125, 337.20953369140625), (772.7925415039062, 731.5386962890625)),\n    ('circle', (75.26021575927734, 353.7659912109375), (211.26466369628906, 485.6578369140625)),\n    ('Pentagon', (197.82093811035156, 192.63380432128906), (629.0830688476562, 607.8374633789062)),\n    ('Racetrack', (399.3763732910156, 37.14316940307617), (734.2215576171875, 357.04290771484375)),\n    ('circle', (191.36500549316406, 624.5045776367188), (327.5169677734375, 757.7999877929688)),\n    ('circle', (540.4708862304688, 566.7578735351562), (676.06787109375, 699.739501953125)),\n    ('Racetrack', (91.32276916503906, 34.96403884887695), (442.41143798828125, 351.8035888671875)),\n    ('circle', (579.9710083007812, 197.84326171875), (715.2356567382812, 331.45751953125)),\n    ('circle', (289.7023010253906, 63.5079460144043), (423.3017883300781, 196.29493713378906)),\n    ('Racetrack', (181.86737060546875, 609.5239868164062), (572.858642578125, 773.0011596679688)),\n    ('Racetrack', (57.61482620239258, 337.6130676269531), (306.7226257324219, 737.0006103515625)),\n]\n\n\n\n\n\n\n\n\n\nConvert Co-ordinates of detected objects into JSON\n\ndetected_objects = []\nfor class_id, conf, bbox in zip(detections.class_id, detections.confidence, detections.xyxy):\n    x1, y1, x2, y2 = bbox.tolist()\n    detected_objects.append({\n        \"class\": my_classes[class_id],\n        \"confidence\": float(conf),\n        \"bbox\": {\n            \"x1\": float(x1),\n            \"y1\": float(y1),\n            \"x2\": float(x2),\n            \"y2\": float(y2)\n        }\n    })\n\nimport json\nprint(json.dumps(detected_objects, indent=2))\n\n[\n  {\n    \"class\": \"Diamond\",\n    \"confidence\": 0.9714662432670593,\n    \"bbox\": {\n      \"x1\": 29.127370834350586,\n      \"y1\": 407.62640380859375,\n      \"x2\": 239.15098571777344,\n      \"y2\": 544.3034057617188\n    }\n  },\n  {\n    \"class\": \"Diamond\",\n    \"confidence\": 0.9698772430419922,\n    \"bbox\": {\n      \"x1\": 28.874351501464844,\n      \"y1\": 227.95535278320312,\n      \"x2\": 238.50436401367188,\n      \"y2\": 363.88623046875\n    }\n  },\n  {\n    \"class\": \"Racetrack\",\n    \"confidence\": 0.9692795872688293,\n    \"bbox\": {\n      \"x1\": 384.4695739746094,\n      \"y1\": 255.876953125,\n      \"x2\": 697.0001220703125,\n      \"y2\": 361.5684509277344\n    }\n  },\n  {\n    \"class\": \"Diamond\",\n    \"confidence\": 0.9654581546783447,\n    \"bbox\": {\n      \"x1\": 29.296884536743164,\n      \"y1\": 47.461517333984375,\n      \"x2\": 238.36622619628906,\n      \"y2\": 183.186767578125\n    }\n  },\n  {\n    \"class\": \"Triangle\",\n    \"confidence\": 0.9631335139274597,\n    \"bbox\": {\n      \"x1\": 844.4630737304688,\n      \"y1\": 72.72488403320312,\n      \"x2\": 1027.826416015625,\n      \"y2\": 192.2169952392578\n    }\n  },\n  {\n    \"class\": \"Triangle\",\n    \"confidence\": 0.96173495054245,\n    \"bbox\": {\n      \"x1\": 844.9676513671875,\n      \"y1\": 233.00119018554688,\n      \"x2\": 1028.5804443359375,\n      \"y2\": 353.65087890625\n    }\n  },\n  {\n    \"class\": \"solid-arrow\",\n    \"confidence\": 0.9547531008720398,\n    \"bbox\": {\n      \"x1\": 694.0907592773438,\n      \"y1\": 131.23728942871094,\n      \"x2\": 886.1165771484375,\n      \"y2\": 313.0189208984375\n    }\n  },\n  {\n    \"class\": \"solid-arrow\",\n    \"confidence\": 0.9540013074874878,\n    \"bbox\": {\n      \"x1\": 698.8170166015625,\n      \"y1\": 304.6488037109375,\n      \"x2\": 886.5786743164062,\n      \"y2\": 482.6002197265625\n    }\n  },\n  {\n    \"class\": \"Triangle\",\n    \"confidence\": 0.9502191543579102,\n    \"bbox\": {\n      \"x1\": 843.9762573242188,\n      \"y1\": 394.875732421875,\n      \"x2\": 1028.1793212890625,\n      \"y2\": 514.8218383789062\n    }\n  },\n  {\n    \"class\": \"solid-arrow\",\n    \"confidence\": 0.9406890869140625,\n    \"bbox\": {\n      \"x1\": 242.09933471679688,\n      \"y1\": 303.71929931640625,\n      \"x2\": 382.69677734375,\n      \"y2\": 490.77899169921875\n    }\n  },\n  {\n    \"class\": \"solid-arrow\",\n    \"confidence\": 0.9405144453048706,\n    \"bbox\": {\n      \"x1\": 242.72161865234375,\n      \"y1\": 101.90926361083984,\n      \"x2\": 385.8294372558594,\n      \"y2\": 311.9975280761719\n    }\n  },\n  {\n    \"class\": \"solid-arrow\",\n    \"confidence\": 0.8978973627090454,\n    \"bbox\": {\n      \"x1\": 697.5059814453125,\n      \"y1\": 293.218994140625,\n      \"x2\": 883.3981323242188,\n      \"y2\": 330.16082763671875\n    }\n  },\n  {\n    \"class\": \"solid-arrow\",\n    \"confidence\": 0.8837128281593323,\n    \"bbox\": {\n      \"x1\": 243.73397827148438,\n      \"y1\": 281.9586181640625,\n      \"x2\": 383.1846618652344,\n      \"y2\": 313.9234619140625\n    }\n  },\n  {\n    \"class\": \"arrow_head\",\n    \"confidence\": 0.8148650527000427,\n    \"bbox\": {\n      \"x1\": 243.56497192382812,\n      \"y1\": 281.25982666015625,\n      \"x2\": 270.8069152832031,\n      \"y2\": 311.5813293457031\n    }\n  },\n  {\n    \"class\": \"arrow_head\",\n    \"confidence\": 0.8067644238471985,\n    \"bbox\": {\n      \"x1\": 243.93441772460938,\n      \"y1\": 462.5649719238281,\n      \"x2\": 272.4416809082031,\n      \"y2\": 490.833251953125\n    }\n  },\n  {\n    \"class\": \"arrow_head\",\n    \"confidence\": 0.795447051525116,\n    \"bbox\": {\n      \"x1\": 862.6205444335938,\n      \"y1\": 292.51361083984375,\n      \"x2\": 885.913818359375,\n      \"y2\": 316.01947021484375\n    }\n  },\n  {\n    \"class\": \"arrow_head\",\n    \"confidence\": 0.7893730401992798,\n    \"bbox\": {\n      \"x1\": 243.89553833007812,\n      \"y1\": 101.54055786132812,\n      \"x2\": 271.93878173828125,\n      \"y2\": 131.6016845703125\n    }\n  },\n  {\n    \"class\": \"arrow_head\",\n    \"confidence\": 0.782149612903595,\n    \"bbox\": {\n      \"x1\": 866.0701293945312,\n      \"y1\": 454.7251281738281,\n      \"x2\": 886.1697387695312,\n      \"y2\": 480.5311584472656\n    }\n  },\n  {\n    \"class\": \"arrow_head\",\n    \"confidence\": 0.7689226865768433,\n    \"bbox\": {\n      \"x1\": 863.8754272460938,\n      \"y1\": 130.90518188476562,\n      \"x2\": 886.8487548828125,\n      \"y2\": 155.73548889160156\n    }\n  }\n]\n\n\n\n\nPaddle OCR\nsaved the output in the OCRoutput folder after applying the OCR: 1. first image saved by after applying the ocr 2. second saved the response in the JSON format of the OCR applied image.\n\npip install langchain==0.0.350\n\nCollecting langchain==0.0.350\n  Downloading langchain-0.0.350-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: PyYAML&gt;=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (6.0.2)\nRequirement already satisfied: SQLAlchemy&lt;3,&gt;=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (2.0.44)\nRequirement already satisfied: aiohttp&lt;4.0.0,&gt;=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (3.13.2)\nCollecting dataclasses-json&lt;0.7,&gt;=0.5.7 (from langchain==0.0.350)\n  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: jsonpatch&lt;2.0,&gt;=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (1.33)\nCollecting langchain-community&lt;0.1,&gt;=0.0.2 (from langchain==0.0.350)\n  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\nCollecting langchain-core&lt;0.2,&gt;=0.1 (from langchain==0.0.350)\n  Downloading langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\nCollecting langsmith&lt;0.1.0,&gt;=0.0.63 (from langchain==0.0.350)\n  Downloading langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\nCollecting numpy&lt;2,&gt;=1 (from langchain==0.0.350)\n  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.0/61.0 kB 6.9 MB/s eta 0:00:00\nRequirement already satisfied: pydantic&lt;3,&gt;=1 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (2.12.3)\nRequirement already satisfied: requests&lt;3,&gt;=2 in /usr/local/lib/python3.12/dist-packages (from langchain==0.0.350) (2.32.4)\nCollecting tenacity&lt;9.0.0,&gt;=8.1.0 (from langchain==0.0.350)\n  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain==0.0.350) (2.6.1)\nRequirement already satisfied: aiosignal&gt;=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain==0.0.350) (1.4.0)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain==0.0.350) (25.4.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain==0.0.350) (1.8.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain==0.0.350) (6.7.0)\nRequirement already satisfied: propcache&gt;=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain==0.0.350) (0.4.1)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp&lt;4.0.0,&gt;=3.8.3-&gt;langchain==0.0.350) (1.22.0)\nCollecting marshmallow&lt;4.0.0,&gt;=3.18.0 (from dataclasses-json&lt;0.7,&gt;=0.5.7-&gt;langchain==0.0.350)\n  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting typing-inspect&lt;1,&gt;=0.4.0 (from dataclasses-json&lt;0.7,&gt;=0.5.7-&gt;langchain==0.0.350)\n  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: jsonpointer&gt;=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch&lt;2.0,&gt;=1.33-&gt;langchain==0.0.350) (3.0.0)\nINFO: pip is looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-community&lt;0.1,&gt;=0.0.2 (from langchain==0.0.350)\n  Downloading langchain_community-0.0.37-py3-none-any.whl.metadata (8.7 kB)\n  Downloading langchain_community-0.0.36-py3-none-any.whl.metadata (8.7 kB)\n  Downloading langchain_community-0.0.35-py3-none-any.whl.metadata (8.7 kB)\n  Downloading langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n  Downloading langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n  Downloading langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\n  Downloading langchain_community-0.0.31-py3-none-any.whl.metadata (8.4 kB)\nINFO: pip is still looking at multiple versions of langchain-community to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_community-0.0.30-py3-none-any.whl.metadata (8.4 kB)\n  Downloading langchain_community-0.0.29-py3-none-any.whl.metadata (8.3 kB)\n  Downloading langchain_community-0.0.28-py3-none-any.whl.metadata (8.3 kB)\n  Downloading langchain_community-0.0.27-py3-none-any.whl.metadata (8.2 kB)\n  Downloading langchain_community-0.0.26-py3-none-any.whl.metadata (8.2 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading langchain_community-0.0.25-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.23-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.22-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.21-py3-none-any.whl.metadata (8.1 kB)\n  Downloading langchain_community-0.0.20-py3-none-any.whl.metadata (8.1 kB)\nINFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\nCollecting langchain-core&lt;0.2,&gt;=0.1 (from langchain==0.0.350)\n  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.51-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.50-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.49-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.48-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.47-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.46-py3-none-any.whl.metadata (5.9 kB)\nINFO: pip is still looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n  Downloading langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.44-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.43-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.41-py3-none-any.whl.metadata (5.9 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading langchain_core-0.1.40-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.39-py3-none-any.whl.metadata (5.9 kB)\n  Downloading langchain_core-0.1.38-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.37-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.36-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.35-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.34-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.33-py3-none-any.whl.metadata (6.0 kB)\nRequirement already satisfied: anyio&lt;5,&gt;=3 in /usr/local/lib/python3.12/dist-packages (from langchain-core&lt;0.2,&gt;=0.1-&gt;langchain==0.0.350) (4.12.0)\n  Downloading langchain_core-0.1.32-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.31-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.30-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.29-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.28-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.27-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.25-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.24-py3-none-any.whl.metadata (6.0 kB)\n  Downloading langchain_core-0.1.23-py3-none-any.whl.metadata (6.0 kB)\nCollecting langsmith&lt;0.1.0,&gt;=0.0.63 (from langchain==0.0.350)\n  Downloading langsmith-0.0.87-py3-none-any.whl.metadata (10 kB)\nCollecting packaging&lt;24.0,&gt;=23.2 (from langchain-core&lt;0.2,&gt;=0.1-&gt;langchain==0.0.350)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic&lt;3,&gt;=1-&gt;langchain==0.0.350) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic&lt;3,&gt;=1-&gt;langchain==0.0.350) (2.41.4)\nRequirement already satisfied: typing-extensions&gt;=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic&lt;3,&gt;=1-&gt;langchain==0.0.350) (4.15.0)\nRequirement already satisfied: typing-inspection&gt;=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic&lt;3,&gt;=1-&gt;langchain==0.0.350) (0.4.2)\nRequirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.12/dist-packages (from requests&lt;3,&gt;=2-&gt;langchain==0.0.350) (3.4.4)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.12/dist-packages (from requests&lt;3,&gt;=2-&gt;langchain==0.0.350) (3.7)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests&lt;3,&gt;=2-&gt;langchain==0.0.350) (2.5.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests&lt;3,&gt;=2-&gt;langchain==0.0.350) (2025.11.12)\nRequirement already satisfied: greenlet&gt;=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy&lt;3,&gt;=1.4-&gt;langchain==0.0.350) (3.3.0)\nCollecting mypy-extensions&gt;=0.3.0 (from typing-inspect&lt;1,&gt;=0.4.0-&gt;dataclasses-json&lt;0.7,&gt;=0.5.7-&gt;langchain==0.0.350)\n  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\nDownloading langchain-0.0.350-py3-none-any.whl (809 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 809.1/809.1 kB 48.7 MB/s eta 0:00:00\nDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\nDownloading langchain_community-0.0.20-py3-none-any.whl (1.7 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 89.1 MB/s eta 0:00:00\nDownloading langchain_core-0.1.23-py3-none-any.whl (241 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 241.2/241.2 kB 30.2 MB/s eta 0:00:00\nDownloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.4/55.4 kB 7.0 MB/s eta 0:00:00\nDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.0/18.0 MB 100.2 MB/s eta 0:00:00\nDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\nDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.9/50.9 kB 6.1 MB/s eta 0:00:00\nDownloading packaging-23.2-py3-none-any.whl (53 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 5.7 MB/s eta 0:00:00\nDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\nInstalling collected packages: tenacity, packaging, numpy, mypy-extensions, typing-inspect, marshmallow, langsmith, dataclasses-json, langchain-core, langchain-community, langchain\n  Attempting uninstall: tenacity\n    Found existing installation: tenacity 9.1.2\n    Uninstalling tenacity-9.1.2:\n      Successfully uninstalled tenacity-9.1.2\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.0.2\n    Uninstalling numpy-2.0.2:\n      Successfully uninstalled numpy-2.0.2\n  Attempting uninstall: langsmith\n    Found existing installation: langsmith 0.4.55\n    Uninstalling langsmith-0.4.55:\n      Successfully uninstalled langsmith-0.4.55\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 1.1.1\n    Uninstalling langchain-core-1.1.1:\n      Successfully uninstalled langchain-core-1.1.1\n  Attempting uninstall: langchain\n    Found existing installation: langchain 1.1.2\n    Uninstalling langchain-1.1.2:\n      Successfully uninstalled langchain-1.1.2\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nshap 0.50.0 requires numpy&gt;=2, but you have numpy 1.26.4 which is incompatible.\njaxlib 0.7.2 requires numpy&gt;=2.0, but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy&lt;2.3.0,&gt;=2; python_version &gt;= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nlanggraph-checkpoint 3.0.1 requires langchain-core&gt;=0.2.38, but you have langchain-core 0.1.23 which is incompatible.\ngoogle-cloud-bigquery 3.38.0 requires packaging&gt;=24.2.0, but you have packaging 23.2 which is incompatible.\npytensor 2.35.1 requires numpy&gt;=2.0, but you have numpy 1.26.4 which is incompatible.\njax 0.7.2 requires numpy&gt;=2.0, but you have numpy 1.26.4 which is incompatible.\nxarray 2025.11.0 requires packaging&gt;=24.1, but you have packaging 23.2 which is incompatible.\ngoogle-adk 1.20.0 requires tenacity&lt;10.0.0,&gt;=9.0.0, but you have tenacity 8.5.0 which is incompatible.\nlanggraph-prebuilt 1.0.5 requires langchain-core&gt;=1.0.0, but you have langchain-core 0.1.23 which is incompatible.\ndb-dtypes 1.4.4 requires packaging&gt;=24.2.0, but you have packaging 23.2 which is incompatible.\nSuccessfully installed dataclasses-json-0.6.7 langchain-0.0.350 langchain-community-0.0.20 langchain-core-0.1.23 langsmith-0.0.87 marshmallow-3.26.1 mypy-extensions-1.1.0 numpy-1.26.4 packaging-23.2 tenacity-8.5.0 typing-inspect-0.9.0\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\n\n\n\nimport cv2\nimport numpy as np\nfrom paddleocr import PaddleOCR\nimport os\n\n\nif file_path.lower().endswith('.pdf'):\n    image_for_ocr_path = output_image_path\nelse:\n    image_for_ocr_path = file_path\n\nimg = cv2.imread(image_for_ocr_path)\n\nkernel = np.array([[0, -1, 0],\n                   [-1, 3, -1],\n                   [0, -1, 0]])\n\nsharpened = cv2.filter2D(img, -1, kernel)\n\nalpha = 0.5\nsharpened = cv2.addWeighted(img, 1 - alpha, sharpened, alpha, 0)\n\nocr = PaddleOCR(\n    use_doc_orientation_classify=False,\n    use_doc_unwarping=False,\n    use_textline_orientation=True,\n    lang='en',\n)\n\nresult = ocr.predict(sharpened)\n\nout_dir = \"OCRoutput\"\nos.makedirs(out_dir, exist_ok=True)\n\nbase_name = os.path.splitext(os.path.basename(file_path))[0]\n\nimg_output_path = os.path.join(out_dir, f\"{base_name}_ocr_res_img.png\")\njson_output_path = os.path.join(out_dir, f\"{base_name}_ocr_res.json\")\n\nfor i, res in enumerate(result):\n    res.print()\n    res.save_to_img(img_output_path)\n    res.save_to_json(json_output_path)\n    break\n\n\nChecking connectivity to the model hosters, this may take a while. To bypass this check, set `DISABLE_MODEL_SOURCE_CHECK` to `True`.\n\nUserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n\nCreating model: ('PP-LCNet_x1_0_textline_ori', None)\n\nUsing official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in `/root/.paddlex/official_models/PP-LCNet_x1_0_textline_ori`.\n\nUserWarning: \n\nThe secret `HF_TOKEN` does not exist in your Colab secrets.\n\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n\nYou will be able to reuse this secret in all of your notebooks.\n\nPlease note that authentication is recommended but still optional to access public models or datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating model: ('PP-OCRv5_server_det', None)\n\nUsing official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in `/root/.paddlex/official_models/PP-OCRv5_server_det`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreating model: ('en_PP-OCRv5_mobile_rec', None)\n\nUsing official model (en_PP-OCRv5_mobile_rec), the model files will be automatically downloaded and saved in `/root/.paddlex/official_models/en_PP-OCRv5_mobile_rec`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n{'res': {'input_path': None, 'page_index': None, 'model_settings': {'use_doc_preprocessor': False, 'use_textline_orientation': True}, 'dt_polys': array([[[471,  87],\n\n        ...,\n\n        [453, 108]],\n\n\n\n       ...,\n\n\n\n       [[365, 713],\n\n        ...,\n\n        [365, 733]]], dtype=int16), 'text_det_params': {'limit_side_len': 64, 'limit_type': 'min', 'thresh': 0.3, 'max_side_limit': 4000, 'box_thresh': 0.6, 'unclip_ratio': 1.5}, 'text_type': 'general', 'textline_orientation_angles': array([0, ..., 0]), 'text_rec_score_thresh': 0.0, 'return_word_box': False, 'rec_texts': ['The void always', '#1', 'stares back.', '#2', 'We rot beautifully.', 'Fragments of', 'Fire: Short', '#5', 'Truths for', 'a', 'Especially you.', 'Heavy Mind', 'Everything', 'Hope is a', 'lends.', 'dangerous', 'illusion', '#3', 'Light', 'lies;', '#4', 'shadows', 'remember.'], 'rec_scores': array([0.99967277, ..., 0.99994493]), 'rec_polys': array([[[471,  87],\n\n        ...,\n\n        [453, 108]],\n\n\n\n       ...,\n\n\n\n       [[365, 713],\n\n        ...,\n\n        [365, 733]]], dtype=int16), 'rec_boxes': array([[453, ..., 225],\n\n       ...,\n\n       [365, ..., 733]], dtype=int16)}}\n\nConnecting to https://paddle-model-ecology.bj.bcebos.com/paddlex/PaddleX3.0/fonts/simfang.ttf ...\n\nDownloading simfang.ttf ...\n\n[==================================================] 100.00%\n\n\n\n\nGet the bbox of the text extracted by the paddleOCR from the json file\n\nimport json\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport os\n\n\nSTANDARD_ANGLES = [0.0, 45.0, 90.0, 135.0, 180.0, 225.0, 270.0, 315.0, 360.0]\nSNAP_TOLERANCE = 10.0\n\n\n\ndef _flatten_poly(poly):\n    arr = np.array(poly, dtype=float)\n    if arr.ndim == 1:\n        arr = arr.reshape(-1, 2)\n    return arr\n\ndef _box_area(b):\n    return max(0, b[2] - b[0]) * max(0, b[3] - b[1])\n\ndef _intersection_area(a, b):\n    ix1, iy1 = max(a[0], b[0]), max(a[1], b[1])\n    ix2, iy2 = min(a[2], b[2]), min(a[3], b[3])\n    return max(0, ix2 - ix1) * max(0, iy2 - iy1)\n\n\n\ndef calculate_rotation_from_polygon(polygon):\n\n    pts = _flatten_poly(polygon)\n    n = pts.shape[0]\n\n    if n &lt; 2:\n        return 0.0\n\n    if n == 2:\n        dx = pts[1, 0] - pts[0, 0]\n        dy = pts[1, 1] - pts[0, 1]\n        return (math.degrees(math.atan2(-dy, dx)) + 360) % 360\n\n    centered = pts - pts.mean(axis=0)\n\n    if np.allclose(centered, 0, atol=1e-6):\n        return 0.0\n\n    _, _, Vt = np.linalg.svd(centered, full_matrices=False)\n    vx, vy = Vt[0]\n    pca_angle = (math.degrees(math.atan2(-vy, vx)) + 360) % 360\n\n\n    x = pts[:, 0]\n    y = pts[:, 1]\n    if len(np.unique(x)) &gt; 1:\n        a, b = np.polyfit(x, y, 1)\n        lf_angle = (math.degrees(math.atan2(-a, 1)) + 360) % 360\n    else:\n        lf_angle = pca_angle\n\n    pca_var = np.var(centered @ np.array([vx, vy]))\n    lf_dir = np.array([1, a]) / np.linalg.norm([1, a]) if len(np.unique(x)) &gt; 1 else np.array([1, 0])\n    lf_var = np.var(centered @ lf_dir)\n\n    best = pca_angle if pca_var &gt;= lf_var else lf_angle\n    return round(best, 2)\n\n\n\ndef snap_to_standard_angle(angle, tolerance=SNAP_TOLERANCE):\n\n    angle = angle % 360\n    best = min(STANDARD_ANGLES, key=lambda s: abs(angle - s))\n    return int(best) if abs(best - angle) &lt;= tolerance else round(angle, 1)\n\n\n\ndef get_text_rotation_for_shape(shape_bbox, ocr_data, threshold=0.25):\n    sx1, sy1, sx2, sy2 = shape_bbox\n\n    weighted_vector = np.array([0.0, 0.0])\n    total_weight = 0\n    matched = False\n\n    for item in ocr_data:\n        tx1, ty1, tx2, ty2 = item[\"bbox\"]\n        text_area = _box_area(item[\"bbox\"])\n        if text_area == 0:\n            continue\n\n        inter = _intersection_area(shape_bbox, item[\"bbox\"])\n        overlap = inter / text_area\n        if overlap &lt; threshold:\n            continue\n\n        matched = True\n        conf = float(item.get(\"confidence\", 1.0))\n\n        ang = calculate_rotation_from_polygon(item.get(\"polygon\", [(tx1, ty1), (tx2, ty2)]))\n\n        rad = math.radians(ang)\n        ux, uy = math.cos(rad), math.sin(rad)\n\n        w = overlap * conf * max(1, text_area)\n        weighted_vector += w * np.array([ux, uy])\n        total_weight += w\n\n    if not matched or total_weight == 0:\n        return None\n\n    mean_vec = weighted_vector / total_weight\n    mean_angle = (math.degrees(math.atan2(mean_vec[1], mean_vec[0])) + 360) % 360\n    mean_angle = round(mean_angle, 2)\n\n    snapped = snap_to_standard_angle(mean_angle)\n\n    return {\"raw_rotation\": mean_angle, \"shape_rotation\": snapped}\n\n\n\ndef process_paddleocr_json(json_path, min_confidence=0.4):\n    processed = []\n\n    try:\n        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n            ocr = json.load(f)\n    except:\n        return []\n\n    texts = ocr.get(\"rec_texts\", [])\n    scores = ocr.get(\"rec_scores\", [])\n    polys = ocr.get(\"rec_polys\", [])\n    orientation = ocr.get(\"textline_orientation_angles\", [])\n\n    N = min(len(texts), len(scores), len(polys))\n\n    for i in range(N):\n        if scores[i] &lt; min_confidence:\n            continue\n\n        poly = _flatten_poly(polys[i])\n        x1, y1 = np.min(poly, axis=0)\n        x2, y2 = np.max(poly, axis=0)\n\n        raw_rot = calculate_rotation_from_polygon(poly)\n        snapped = snap_to_standard_angle(raw_rot)\n\n        processed.append({\n            \"text\": texts[i],\n            \"confidence\": float(scores[i]),\n            \"bbox\": (int(x1), int(y1), int(x2), int(y2)),\n            \"polygon\": polys[i],\n            \"paddleocr_orientation\": orientation[i] if i &lt; len(orientation) else None,\n            \"polygon_rotation\": raw_rot,\n            \"shape_rotation\": snapped\n        })\n\n    return processed\n\n\n\ndef plot_ocr_detections(data):\n    if not data:\n        print(\"No data.\")\n        return\n\n    fig, ax = plt.subplots(1, figsize=(14, 12))\n    ax.set_facecolor(\"black\")\n\n    for item in data:\n        x1, y1, x2, y2 = item[\"bbox\"]\n        raw = item[\"polygon_rotation\"]\n        snap = item[\"shape_rotation\"]\n\n        ax.add_patch(\n            patches.Rectangle((x1, y1), x2 - x1, y2 - y1, ec=\"r\", fc=\"none\", lw=1)\n        )\n        ax.text(x1, y1 - 5, f\"{item['text']} ({raw}° → {snap}°)\",\n                fontsize=6, color=\"white\", backgroundcolor=\"red\")\n\n    xs = [d[\"bbox\"][0] for d in data] + [d[\"bbox\"][2] for d in data]\n    ys = [d[\"bbox\"][1] for d in data] + [d[\"bbox\"][3] for d in data]\n\n    ax.set_xlim(min(xs) - 10, max(xs) + 10)\n    ax.set_ylim(max(ys) + 10, min(ys) - 10)\n    ax.invert_yaxis()\n    ax.set_aspect(\"equal\")\n    plt.axis(\"off\")\n    plt.show()\n\n\n\nbase_name = os.path.splitext(os.path.basename(file_path))[0]\njson_output_path = f\"OCRoutput/{base_name}_ocr_res.json\"\n\nif os.path.exists(json_output_path):\n    final_processed_data = process_paddleocr_json(json_output_path)\n    if final_processed_data:\n        print(\"Processed OCR data (full 360° rotation):\")\n        print(\"-\" * 70)\n        for item in final_processed_data:\n            print(f\"{item['text'][:20]:&lt;20} | Raw: {item['polygon_rotation']:6.1f}° → Rounded: {item['shape_rotation']}°\")\n        print(\"-\" * 70)\n        plot_ocr_detections(final_processed_data)\nelse:\n    print(f\"JSON not found: {json_output_path}\")\n\nProcessed OCR data (full 360° rotation):\n----------------------------------------------------------------------\nThe void always      | Raw:  319.0° → Rounded: 315°\n#1                   | Raw:    0.0° → Rounded: 0°\nstares back.         | Raw:  321.4° → Rounded: 315°\n#2                   | Raw:    0.0° → Rounded: 0°\nWe rot beautifully.  | Raw:   39.5° → Rounded: 45°\nFragments of         | Raw:    0.0° → Rounded: 0°\nFire: Short          | Raw:  359.4° → Rounded: 360°\n#5                   | Raw:    1.9° → Rounded: 0°\nTruths for           | Raw:    0.0° → Rounded: 0°\na                    | Raw:    0.0° → Rounded: 0°\nEspecially you.      | Raw:  285.8° → Rounded: 285.8°\nHeavy Mind           | Raw:    0.0° → Rounded: 0°\nEverything           | Raw:  284.0° → Rounded: 284.0°\nHope is a            | Raw:  249.2° → Rounded: 249.2°\nlends.               | Raw:  285.0° → Rounded: 285.0°\ndangerous            | Raw:  250.6° → Rounded: 250.6°\nillusion             | Raw:  248.1° → Rounded: 248.1°\n#3                   | Raw:    0.0° → Rounded: 0°\nLight                | Raw:  357.4° → Rounded: 360°\nlies;                | Raw:    0.0° → Rounded: 0°\n#4                   | Raw:    0.0° → Rounded: 0°\nshadows              | Raw:  358.9° → Rounded: 360°\nremember.            | Raw:    0.0° → Rounded: 0°\n----------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\n\nSpatial Relationship\nMain pipeline where find out the main relationship of the arrows with the shapes and the detections of the arrow’s endpoints\n\nimport cv2\nimport matplotlib.pyplot as plt\nfrom skimage.morphology import skeletonize\nfrom collections import deque\nimport itertools\nimport math\n\n# ==============================================================================\n# ---------- HELPER FUNCTIONS  ----------\n# ==============================================================================\n\ndef show_image(title, img, cmap=None):\n    plt.figure(figsize=(4, 4))\n    if len(img.shape) == 2: plt.imshow(img, cmap=cmap or \"gray\")\n    else: plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    plt.title(title); plt.axis(\"off\"); plt.show()\n\ndef crop_image_region(image, top_left, bottom_right):\n    x1, y1 = map(int, map(round, top_left))\n    x2, y2 = map(int, map(round, bottom_right))\n    return image[y1:y2, x1:x2]\n\ndef convert_to_binary_mask(cropped_img):\n    if cropped_img.size == 0:\n        return np.array([], dtype=np.uint8)\n    gray = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2GRAY)\n    blurred = cv2.GaussianBlur(gray, (3,3), 0)\n\n    edges = cv2.Canny(blurred, threshold1=30, threshold2=100)\n\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n    closed = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel, iterations=1)\n\n    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    if np.mean(thresh) &gt; 127:\n        thresh = cv2.bitwise_not(thresh)\n    combined_mask = cv2.bitwise_or(closed, thresh)\n    return combined_mask\n\n\ndef connect_dotted_simple_morph(binary_mask, kernel_size=5, iterations=2):\n\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n    closed_mask = cv2.morphologyEx(binary_mask, cv2.MORPH_CLOSE, kernel, iterations=1)\n    connected_mask = cv2.dilate(closed_mask, kernel, iterations=iterations)\n    return connected_mask\n\n\ndef find_farthest_points_in_contour(contour):\n    max_dist_sq, best_pair = -1, None\n    points = contour.squeeze(axis=1)\n    if len(points) &lt; 2: return None\n    for p1, p2 in itertools.combinations(points, 2):\n        dist_sq = (p1[0] - p2[0])**2 + (p1[1] - p2[1])**2\n        if dist_sq &gt; max_dist_sq: max_dist_sq, best_pair = dist_sq, (tuple(p1), tuple(p2))\n    return best_pair\n\ndef interpolate_dashed_arrows(binary_mask, max_gap_ratio=0.5, thickness=2):\n    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    contours = [c for c in contours if cv2.contourArea(c) &gt; 3]\n\n    if len(contours) &lt; 2:\n        return binary_mask\n\n    dash_endpoints = []\n    for cnt in contours:\n        end_pts = find_farthest_points_in_contour(cnt)\n        if end_pts:\n            dash_endpoints.append(list(end_pts))\n\n    dash_lengths = [math.dist(ep[0], ep[1]) for ep in dash_endpoints if ep]\n    if not dash_lengths:\n        return binary_mask\n    max_connect_dist = np.median(dash_lengths) * (1 + max_gap_ratio)\n\n    output_mask = binary_mask.copy()\n\n    for i, eps1 in enumerate(dash_endpoints):\n        for ep1 in eps1:\n            min_dist = float('inf')\n            best_ep = None\n            for j, eps2 in enumerate(dash_endpoints):\n                if i == j:\n                    continue\n                for ep2 in eps2:\n                    dist = math.dist(ep1, ep2)\n                    if dist &lt; min_dist:\n                        min_dist = dist\n                        best_ep = ep2\n            if best_ep and min_dist &lt; max_connect_dist:\n                cv2.line(output_mask, ep1, best_ep, 255, thickness=thickness)\n\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (6, 5))\n    output_mask = cv2.dilate(output_mask, kernel, iterations=2)\n\n    return output_mask\n\n\ndef extract_skeleton(binary_mask):\n    return skeletonize(binary_mask &gt; 0).astype(np.uint8) * 255\n\n\ndef get_skeleton_graph_nodes(skeleton):\n    endpoints, junctions = [], []\n    h, w = skeleton.shape\n    padded_skeleton = np.pad(skeleton, 1, 'constant')\n    for y_pad in range(1, h + 1):\n        for x_pad in range(1, w + 1):\n            if padded_skeleton[y_pad, x_pad] &gt; 0:\n                num_neighbors = np.sum(padded_skeleton[y_pad-1:y_pad+2, x_pad-1:x_pad+2] &gt; 0) - 1\n                if num_neighbors == 1: endpoints.append((x_pad - 1, y_pad - 1))\n                elif num_neighbors &gt; 2: junctions.append((x_pad - 1, y_pad - 1))\n    return endpoints, junctions\n\n\ndef find_farthest_points_euclidean(binary_mask):\n    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    if not contours: return []\n    all_points = np.vstack([cnt.squeeze(axis=1) for cnt in contours if cnt.ndim &gt; 2])\n    if len(all_points) &lt; 2: return []\n    return find_farthest_points_in_contour(np.array(all_points).reshape(-1, 1, 2))\n\n\ndef check_bbox_intersection(box1_tl, box1_br, box2_tl, box2_br):\n    return not (box1_br[0] &lt; box2_tl[0] or box1_tl[0] &gt; box2_br[0] or box1_br[1] &lt; box2_tl[1] or box1_tl[1] &gt; box2_br[1])\n\n\ndef simple_bridge_by_interpolation(binary_mask, ocr_bbox_local, thickness=1):\n\n    h, w = binary_mask.shape[:2]\n    x1, y1, x2, y2 = map(int, ocr_bbox_local)\n    x1, y1 = max(0, x1), max(0, y1)\n    x2, y2 = min(w, x2), min(h, y2)\n    if x2 &lt;= x1 or y2 &lt;= y1:\n        return binary_mask.copy()\n\n    mask_no_text = binary_mask.copy()\n    mask_no_text[y1:y2, x1:x2] = 0\n\n    pts = np.column_stack(np.where(mask_no_text &gt; 0))\n    if pts.size == 0:\n        return binary_mask.copy()\n\n    left_pts = pts[pts[:, 1] &lt; x1]\n    right_pts = pts[pts[:, 1] &gt; x2]\n\n    if left_pts.size and right_pts.size:\n        left_idx = np.argmax(left_pts[:, 1])\n        right_idx = np.argmin(right_pts[:, 1])\n        left_p = (int(left_pts[left_idx, 1]), int(left_pts[left_idx, 0]))\n        right_p = (int(right_pts[right_idx, 1]), int(right_pts[right_idx, 0]))\n        p1, p2 = left_p, right_p\n    else:\n        top_pts = pts[pts[:, 0] &lt; y1]\n        bottom_pts = pts[pts[:, 0] &gt; y2]\n        if top_pts.size and bottom_pts.size:\n            top_idx = np.argmax(top_pts[:, 0])\n            bot_idx = np.argmin(bottom_pts[:, 0])\n            top_p = (int(top_pts[top_idx, 1]), int(top_pts[top_idx, 0]))\n            bot_p = (int(bottom_pts[bot_idx, 1]), int(bottom_pts[bot_idx, 0]))\n            p1, p2 = top_p, bot_p\n        else:\n            coords = np.column_stack((pts[:,1], pts[:,0]))\n            max_d = -1; best = None\n            for i in range(len(coords)):\n                for j in range(i+1, len(coords)):\n                    d = (coords[i,0]-coords[j,0])**2 + (coords[i,1]-coords[j,1])**2\n                    if d &gt; max_d:\n                        max_d = d\n                        best = (tuple(coords[i].tolist()), tuple(coords[j].tolist()))\n            if not best:\n                return binary_mask.copy()\n            p1, p2 = best\n\n    out = mask_no_text.copy()\n    dist = int(math.hypot(p2[0]-p1[0], p2[1]-p1[1]))\n    if dist &lt; 2:\n        cv2.line(out, p1, p2, 255, thickness=max(1, thickness))\n        return out\n\n    n = max(dist, 2)\n    xs = np.linspace(p1[0], p2[0], n)\n    ys = np.linspace(p1[1], p2[1], n)\n    for x, y in zip(xs, ys):\n        cx, cy = int(round(x)), int(round(y))\n        x0, x1_ = max(0, cx - thickness), min(w, cx + thickness + 1)\n        y0, y1_ = max(0, cy - thickness), min(h, cy + thickness + 1)\n        out[y0:y1_, x0:x1_] = 255\n\n    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n    out = cv2.dilate(out, kernel, iterations=1)\n    return out\n\ndef bbox_iou(boxA, boxB):\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    interArea = max(0, xB - xA) * max(0, yB - yA)\n    if interArea == 0:\n        return 0.0\n    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n    return interArea / float(boxAArea + boxBArea - interArea)\n\ndef pixel_level_ocr_interference(binary_arrow_mask, ocr_bbox, crop_origin, coverage_threshold=0.05):\n    crop_x1, crop_y1 = crop_origin\n    h, w = binary_arrow_mask.shape\n\n    ox1, oy1, ox2, oy2 = ocr_bbox\n    local_x1 = max(0, int(ox1 - crop_x1))\n    local_y1 = max(0, int(oy1 - crop_y1))\n    local_x2 = min(w, int(ox2 - crop_x1))\n    local_y2 = min(h, int(oy2 - crop_y1))\n\n    if local_x1 &gt;= local_x2 or local_y1 &gt;= local_y2:\n        return False\n\n    arrow_pixels = np.count_nonzero(binary_arrow_mask)\n    if arrow_pixels == 0:\n        return False\n\n    overlap_region = binary_arrow_mask[local_y1:local_y2, local_x1:local_x2]\n    overlap_count = np.count_nonzero(overlap_region)\n    coverage_ratio = overlap_count / arrow_pixels\n    return coverage_ratio &gt; coverage_threshold\n\ndef mask_overlapping_ocr_regions(binary_mask, ocr_intrusions, crop_origin):\n    mask = binary_mask.copy()\n    crop_x1, crop_y1 = crop_origin\n    h, w = mask.shape\n\n    for ocr in ocr_intrusions:\n        ox1, oy1, ox2, oy2 = ocr['bbox']\n        local_x1 = max(0, int(ox1 - crop_x1))\n        local_y1 = max(0, int(oy1 - crop_y1))\n        local_x2 = min(w, int(ox2 - crop_x1))\n        local_y2 = min(h, int(oy2 - crop_y1))\n        if local_x1 &lt; local_x2 and local_y1 &lt; local_y2:\n            mask[local_y1:local_y2, local_x1:local_x2] = 0\n    return mask\n\ndef match_arrowhead_to_endpoint(arrowhead_bboxes, endpoints, crop_origin):\n\n    if not arrowhead_bboxes:\n        return None\n\n    crop_x, crop_y = crop_origin\n    arrowhead_centers = []\n    for tl, br in arrowhead_bboxes:\n        cx = (tl[0] + br[0]) / 2 - crop_x\n        cy = (tl[1] + br[1]) / 2 - crop_y\n        arrowhead_centers.append((cx, cy))\n\n    for center in arrowhead_centers:\n        for ep in endpoints:\n\n            if abs(ep[0] - center[0]) &lt; 15 and abs(ep[1] - center[1]) &lt; 12:\n                return ep\n    return None\n\n# =================================================================================\n# ---------- FOR INTERSECTING ARROWS ----------\n# =================================================================================\n\ndef bfs_get_path(skeleton, start_xy, end_xy):\n\n\n    h, w = skeleton.shape\n    start_rc, end_rc = (start_xy[1], start_xy[0]), (end_xy[1], end_xy[0])\n\n    if not (0 &lt;= start_rc[0] &lt; h and 0 &lt;= start_rc[1] &lt; w and skeleton[start_rc] &gt; 0): return None\n    if not (0 &lt;= end_rc[0] &lt; h and 0 &lt;= end_rc[1] &lt; w and skeleton[end_rc] &gt; 0): return None\n\n    q = deque([(start_rc, [start_xy])])\n    visited = {start_rc}\n\n    while q:\n        (r, c), path = q.popleft()\n\n        if (r, c) == end_rc:\n            return path\n\n        for dr, dc in itertools.product([-1, 0, 1], repeat=2):\n            if dr == 0 and dc == 0: continue\n            nr, nc = r + dr, c + dc\n\n            if 0 &lt;= nr &lt; h and 0 &lt;= nc &lt; w and skeleton[nr, nc] &gt; 0 and (nr, nc) not in visited:\n                visited.add((nr, nc))\n                new_path = path + [(nc, nr)]\n                q.append(((nr, nc), new_path))\n\n    return None\n\ndef find_best_arrow_by_straightness(binary_mask, min_path_length=20):\n\n    skeleton = extract_skeleton(binary_mask)\n    if np.count_nonzero(skeleton) &lt; min_path_length:\n        return None\n\n    endpoints, _ = get_skeleton_graph_nodes(skeleton)\n\n    if len(endpoints) &lt; 2:\n        return find_farthest_points_euclidean(binary_mask)\n\n    candidate_paths = []\n    for p1, p2 in itertools.combinations(endpoints, 2):\n        path = bfs_get_path(skeleton, p1, p2)\n\n        if path and len(path) &gt;= min_path_length:\n            path_length = len(path)\n            euclidean_dist = math.dist(p1, p2)\n\n            straightness = euclidean_dist / path_length\n\n            score = path_length * straightness\n\n            candidate_paths.append({\n                \"endpoints\": [p1, p2],\n                \"score\": score,\n                \"length\": path_length\n            })\n\n    if not candidate_paths:\n        return None\n\n    best_path = max(candidate_paths, key=lambda x: x['score'])\n    return best_path['endpoints']\n\n\n# ==============================================================================\n# ----------   MAIN PIPELINE FUNCTION  ----------\n# ==============================================================================\n\ndef skeleton_has_cycle(skeleton):\n\n    h, w = skeleton.shape\n    visited = np.zeros((h, w), dtype=bool)\n\n    def neighbors(r, c):\n        for dr in [-1, 0, 1]:\n            for dc in [-1, 0, 1]:\n                if dr == 0 and dc == 0:\n                    continue\n                nr, nc = r + dr, c + dc\n                if 0 &lt;= nr &lt; h and 0 &lt;= nc &lt; w and skeleton[nr, nc] &gt; 0:\n                    yield nr, nc\n\n    def dfs(r, c, pr, pc):\n        visited[r, c] = True\n        for nr, nc in neighbors(r, c):\n            if not visited[nr, nc]:\n                if dfs(nr, nc, r, c):\n                    return True\n            elif (nr, nc) != (pr, pc):\n                return True\n        return False\n\n    for y in range(h):\n        for x in range(w):\n            if skeleton[y, x] &gt; 0 and not visited[y, x]:\n                if dfs(y, x, -1, -1):\n                    return True\n    return False\n\ndef find_farthest_geodesic_endpoints(skeleton):\n    endpoints, _ = get_skeleton_graph_nodes(skeleton)\n    if len(endpoints) &lt; 2: return None\n    max_length, best_pair = -1, None\n    for p1, p2 in itertools.combinations(endpoints, 2):\n        path = bfs_get_path(skeleton, p1, p2)\n        if path and len(path) &gt; max_length:\n            max_length, best_pair = len(path), (p1, p2)\n    return best_pair\n\n\ndef dilate_ocr_regions(binary_mask, ocr_intrusions, crop_origin, vert_expand=1, horiz_expand=1, vert_iterations=1, horiz_iterations=1, erosion_kernel_size=3, erosion_iterations=1):\n    mask = np.zeros_like(binary_mask)\n    crop_x1, crop_y1 = crop_origin\n    h, w = mask.shape\n    for ocr in ocr_intrusions:\n        ox1, oy1, ox2, oy2 = ocr['bbox']\n        local_x1 = max(0, int(ox1 - crop_x1))\n        local_y1 = max(0, int(oy1 - crop_y1))\n        local_x2 = min(w, int(ox2 - crop_x1))\n        local_y2 = min(h, int(oy2 - crop_y1))\n        if local_x1 &lt; local_x2 and local_y1 &lt; local_y2:\n            mask[local_y1:local_y2, local_x1:local_x2] = 255\n\n    vert_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, vert_expand))\n    horiz_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (horiz_expand, 1))\n\n    mask = cv2.dilate(mask, vert_kernel, iterations=vert_iterations)\n    mask = cv2.dilate(mask, horiz_kernel, iterations=horiz_iterations)\n\n    combined = cv2.bitwise_or(binary_mask, mask)\n\n    erosion_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (erosion_kernel_size, erosion_kernel_size))\n    combined = cv2.erode(combined, erosion_kernel, iterations=erosion_iterations)\n\n    return combined\n\ndef find_skeleton_points_on_bbox(skeleton, bbox_tl, bbox_br, margin=2):\n\n    points = np.column_stack(np.where(skeleton &gt; 0))\n    x_min, y_min = bbox_tl\n    x_max, y_max = bbox_br\n    candidate_points = []\n    for y, x in points:\n        near_left = abs(x - x_min) &lt;= margin and y_min &lt;= y &lt;= y_max\n        near_right = abs(x - x_max) &lt;= margin and y_min &lt;= y &lt;= y_max\n        near_top = abs(y - y_min) &lt;= margin and x_min &lt;= x &lt;= x_max\n        near_bottom = abs(y - y_max) &lt;= margin and x_min &lt;= x &lt;= x_max\n        if near_left or near_right or near_top or near_bottom:\n            candidate_points.append((x, y))\n    return candidate_points\n\n\n\ndef debug_stage(image, detections, ocr_results):\n    vectors = []\n    arrow_labels = [\"dashed-arrow\", \"dotted-arrow\", \"solid-arrow\"]\n\n    non_arrow_labels = [\"Diamond\"]\n\n    arrows = [d for d in detections if d[0] in arrow_labels]\n    interfering_objects = [d for d in detections if d[0] in non_arrow_labels]\n\n    for idx, (label, top_left, bottom_right) in enumerate(arrows):\n        print(f\"\\n=== Processing Arrow {idx} ({label}) ===\")\n        crop_x1, crop_y1 = int(top_left[0]), int(top_left[1])\n        crop = crop_image_region(image, top_left, bottom_right)\n        if crop.size == 0:\n            continue\n        show_image(f\"Crop {idx}\", crop)\n\n        binary_mask = convert_to_binary_mask(crop)\n        if binary_mask.size == 0:\n            continue\n\n        processed_mask = binary_mask.copy()\n        endpoints = None\n\n        is_arrow_arrow_overlap = any(\n            idx != other_idx and check_bbox_intersection(top_left, bottom_right, other_tl, other_br)\n            for other_idx, (_, other_tl, other_br) in enumerate(arrows)\n        )\n\n        ocr_intrusions = []\n        if ocr_results:\n            for ocr in ocr_results:\n                if bbox_iou((top_left[0], top_left[1], bottom_right[0], bottom_right[1]), ocr['bbox']) &gt; 0.02:\n                    if pixel_level_ocr_interference(processed_mask, ocr['bbox'], (crop_x1, crop_y1), coverage_threshold=0.02):\n                        ocr_intrusions.append(ocr)\n\n        ocr_interrupted = len(ocr_intrusions) &gt; 0\n\n        if not ocr_interrupted:\n            if label == \"dashed-arrow\":\n                processed_mask = interpolate_dashed_arrows(processed_mask)\n            elif label == \"dotted-arrow\":\n                processed_mask = connect_dotted_simple_morph(processed_mask, kernel_size=5, iterations=2)\n\n        if ocr_interrupted:\n            print(\"--&gt; Strategy: OCR interruption detected. Masking OCR and applying interpolation on all.\")\n            processed_mask = mask_overlapping_ocr_regions(processed_mask, ocr_intrusions, (crop_x1, crop_y1))\n            for ocr in ocr_intrusions:\n                ox1, oy1, ox2, oy2 = ocr['bbox']\n                o_local = (ox1 - crop_x1, oy1 - crop_y1, ox2 - crop_x1, oy2 - crop_y1)\n\n            processed_mask = dilate_ocr_regions(\n                processed_mask, ocr_intrusions, (crop_x1, crop_y1),\n                vert_expand=5, horiz_expand=10, vert_iterations=3, horiz_iterations=1\n            )\n            processed_mask = connect_dotted_simple_morph(processed_mask, kernel_size=6)\n\n            skeleton = extract_skeleton(processed_mask)\n            if skeleton_has_cycle(skeleton):\n                print(\"Cycle detected in skeleton within OCR interrupted region\")\n\n                intersecting_shapes = [d for d in interfering_objects if check_bbox_intersection(top_left, bottom_right, d[1], d[2])]\n                all_intersection_points = []\n                for shape_tl, shape_br in [d[1:3] for d in intersecting_shapes]:\n                    local_shape_tl = (shape_tl[0] - crop_x1, shape_tl[1] - crop_y1)\n                    local_shape_br = (shape_br[0] - crop_x1, shape_br[1] - crop_y1)\n                    points_on_bbox = find_skeleton_points_on_bbox(\n                        skeleton, local_shape_tl, local_shape_br, margin=2\n                    )\n                    all_intersection_points.extend(points_on_bbox)\n\n                if len(all_intersection_points) &gt;= 2:\n                    max_dist = -1\n                    endpoints = None\n                    for p1, p2 in itertools.combinations(all_intersection_points, 2):\n                        dist = math.dist(p1, p2)\n                        if dist &gt; max_dist:\n                            max_dist = dist\n                            endpoints = [p1, p2]\n                    endpoints = [(x + crop_x1, y + crop_y1) for x, y in endpoints]\n                else:\n                    endpoints = find_best_arrow_by_straightness(processed_mask)\n                    if endpoints and len(endpoints) == 2:\n                        endpoints = [(x + crop_x1, y + crop_y1) for x, y in endpoints]\n            else:\n                endpoints = find_best_arrow_by_straightness(processed_mask)\n                if not endpoints or len(endpoints) &lt; 2:\n                    endpoints = find_farthest_points_euclidean(processed_mask)\n                if endpoints:\n                    endpoints = [(x + crop_x1, y + crop_y1) for x, y in endpoints]\n\n        elif is_arrow_arrow_overlap:\n            print(\"--&gt; Strategy: Overlap detected. Using straightness scoring.\")\n            endpoints = find_best_arrow_by_straightness(processed_mask)\n            if not endpoints:\n                print(\"--&gt; Scoring failed. Falling back to farthest points.\")\n                endpoints = find_farthest_points_euclidean(processed_mask)\n            if endpoints:\n                endpoints = [(x + crop_x1, y + crop_y1) for x, y in endpoints]\n        else:\n            print(\"--&gt; Strategy: General arrow analysis with straightness scoring.\")\n            endpoints = find_best_arrow_by_straightness(processed_mask)\n            if endpoints:\n                endpoints = [(x + crop_x1, y + crop_y1) for x, y in endpoints]\n\n        show_image(f\"Final Processed Mask {idx}\", processed_mask, cmap=\"gray\")\n\n        if not endpoints or len(endpoints) &lt; 2:\n            print(f\"-&gt; Arrow {idx}: FAILED to find 2 endpoints.\")\n            continue\n\n        arrowhead_detections = [d for d in detections if d[0] == \"arrow_head\" and check_bbox_intersection(top_left, bottom_right, d[1], d[2])]\n        arrowhead_bboxes = [(d[1], d[2]) for d in arrowhead_detections]\n\n        matched_tail = match_arrowhead_to_endpoint(arrowhead_bboxes, endpoints, (crop_x1, crop_y1))\n\n        if matched_tail is not None:\n            tail_global = matched_tail\n            head_global = endpoints[1] if endpoints[0] == tail_global else endpoints[0]\n        else:\n            width = bottom_right[0] - top_left[0]\n            height = bottom_right[1] - top_left[1]\n            if width &gt;= height:\n                tail_global, head_global = sorted(endpoints, key=lambda e: e[0])\n            else:\n                tail_global, head_global = sorted(endpoints, key=lambda e: e[1])\n\n        tail_local = (tail_global[0] - crop_x1, tail_global[1] - crop_y1)\n        head_local = (head_global[0] - crop_x1, head_global[1] - crop_y1)\n\n        vis_endpoints = crop.copy()\n        cv2.circle(vis_endpoints, (int(head_local[0]), int(head_local[1])), 5, (0, 0, 255), -1)\n        cv2.circle(vis_endpoints, (int(tail_local[0]), int(tail_local[1])), 5, (0, 255, 0), -1)\n        show_image(f\"Endpoints {idx}\", vis_endpoints)\n\n        vectors.append({\n            \"tail\": tail_global, \"head\": head_global,\n            \"bbox\": (top_left, bottom_right), \"label\": label\n        })\n\n    return vectors\n\n\n\nimage = cv2.imread(file_path)\ndetections = detections_list\nvectors = debug_stage(image, detections,final_processed_data)\nprint(\"\\n=== Results from Full Pipeline ===\")\nfor v in vectors:\n  print(f\"Arrow: Tail={v['tail']} → Head={v['head']} -&gt; Label={v['label']} \")\n\n\n=== Results from Full Pipeline ===\n\n\n\nfrom scipy.interpolate import splprep, splev\n\ndef show_image(title, img, cmap=None):\n    plt.figure(figsize=(12, 12))\n    if len(img.shape) == 2:\n        plt.imshow(img, cmap=cmap or \"gray\")\n    else:\n        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    plt.title(title, fontsize=16)\n    plt.axis(\"off\")\n    plt.show()\n\ndef point_to_bbox_distance(point, top_left, bottom_right):\n    px, py = point\n    x1, y1 = top_left\n    x2, y2 = bottom_right\n\n    if x1 &lt;= px &lt;= x2 and y1 &lt;= py &lt;= y2:\n        return 0.0\n\n    dx = max(x1 - px, 0, px - x2)\n    dy = max(y1 - py, 0, py - y2)\n    return math.sqrt(dx*dx + dy*dy)\n\ndef nearest_point_on_bbox(point, top_left, bottom_right):\n    px, py = point\n    x1, y1 = top_left\n    x2, y2 = bottom_right\n    cx = min(max(px, x1), x2)\n    cy = min(max(py, y1), y2)\n    return (int(round(cx)), int(round(cy)))\n\ndef find_nearest_shape_bbox(point, shapes, max_distance=None):\n\n    if not shapes:\n        return None, None, None\n\n    min_dist = float('inf')\n    nearest_shape = None\n    nearest_pt = None\n\n    for shape in shapes:\n        label, top_left, bottom_right = shape\n        pt_on_bbox = nearest_point_on_bbox(point, top_left, bottom_right)\n        dist = math.hypot(pt_on_bbox[0] - point[0], pt_on_bbox[1] - point[1])\n\n        if dist &lt; min_dist:\n            min_dist = dist\n            nearest_shape = shape\n            nearest_pt = pt_on_bbox\n\n    if max_distance is not None and min_dist &gt; max_distance:\n        return None, None, None\n\n    return nearest_shape, nearest_pt, min_dist\n\n\ndef draw_connections_on_image(image, connections):\n    vis_image = image.copy()\n\n    ARROW_COLOR = (0, 255, 0)       # green\n    HEAD_COLOR = (255, 0, 0)        # red\n    TAIL_COLOR = (0, 255, 255)      # cyan\n    CONNECTION_COLOR = (255, 0, 255) # magenta\n\n    for conn in connections:\n        head = conn[\"head\"]\n        tail = conn[\"tail\"]\n\n\n        cv2.arrowedLine(vis_image, tail, head, ARROW_COLOR, 2, tipLength=0.04)\n\n\n        cv2.circle(vis_image, head, 8, HEAD_COLOR, -1)\n        cv2.circle(vis_image, tail, 8, TAIL_COLOR, -1)\n\n        if conn.get(\"head_connected_to\") is not None:\n            head_pt = conn.get(\"head_connection_point\")\n            if head_pt is None:\n\n                _, tl, br = conn[\"head_connected_to\"]\n                head_pt = (int((tl[0] + br[0]) / 2), int((tl[1] + br[1]) / 2))\n            cv2.line(vis_image, head, head_pt, CONNECTION_COLOR, 2, cv2.LINE_AA)\n\n            cv2.circle(vis_image, head_pt, 5, CONNECTION_COLOR, -1)\n\n        if conn.get(\"tail_connected_to\") is not None:\n            tail_pt = conn.get(\"tail_connection_point\")\n            if tail_pt is None:\n                _, tl, br = conn[\"tail_connected_to\"]\n                tail_pt = (int((tl[0] + br[0]) / 2), int((tl[1] + br[1]) / 2))\n            cv2.line(vis_image, tail, tail_pt, CONNECTION_COLOR, 2, cv2.LINE_AA)\n            cv2.circle(vis_image, tail_pt, 5, CONNECTION_COLOR, -1)\n\n    return vis_image\n\n\n\ndef establish_connections(vectors, all_detections):\n    labels_to_exclude = [\"dashed-arrow\", \"dotted-arrow\", \"solid-arrow\", \"arrow_head\"]\n    shape_detections = [d for d in all_detections if d[0] not in labels_to_exclude]\n\n    connections = []\n\n    for vector in vectors:\n        head_point = vector[\"head\"]\n        tail_point = vector[\"tail\"]\n\n        MAX_CONNECTION_DISTANCE = 44\n\n        head_shape, head_pt_on_bbox, head_dist = find_nearest_shape_bbox(\n            head_point, shape_detections, max_distance=MAX_CONNECTION_DISTANCE\n        )\n        tail_shape, tail_pt_on_bbox, tail_dist = find_nearest_shape_bbox(\n            tail_point, shape_detections, max_distance=MAX_CONNECTION_DISTANCE\n        )\n\n        connections.append({\n            \"head\": head_point,\n            \"tail\": tail_point,\n            \"head_connected_to\": head_shape,\n            \"head_connection_point\": head_pt_on_bbox,\n            \"head_connection_dist\": head_dist,\n            \"tail_connected_to\": tail_shape,\n            \"tail_connection_point\": tail_pt_on_bbox,\n            \"tail_connection_dist\": tail_dist,\n            \"original_label\": vector.get(\"label\", \"\")\n        })\n\n    return connections\n\n\n\narrow_connections = establish_connections(vectors, detections)\n\n\nprint(\"\\n--- ARROW CONNECTION RESULTS ---\")\nfor i, conn in enumerate(arrow_connections):\n    head_label = conn['head_connected_to'][0] if conn['head_connected_to'] else 'None'\n    tail_label = conn['tail_connected_to'][0] if conn['tail_connected_to'] else 'None'\n    print(\n        f\"Arrow {i} ({conn['original_label']}):\\n\"\n        f\"  - TAIL at {conn['tail']} connects to -&gt; {tail_label}\\n\"\n        f\"  - HEAD at {conn['head']} connects to -&gt; {head_label}\\n\"\n    )\n\nvisualization_image = draw_connections_on_image(image, arrow_connections)\n\nshow_image(\"Final Arrow Connections\", visualization_image)\n\n\n--- ARROW CONNECTION RESULTS ---\n\n\n\n\n\n\n\n\n\n\n\nAngle Prediction of shapes\nRun once when the notebook is run from start\n\n\n\n/content/sam2\n\n\n\ndef get_text_rotation_for_shape(shape_bbox, ocr_data, threshold=0.5):\n\n    sx1, sy1, sx2, sy2 = shape_bbox\n    shape_area = (sx2 - sx1) * (sy2 - sy1)\n\n    best_overlap = 0\n    best_rotation = None\n\n    for ocr_item in ocr_data:\n        tx1, ty1, tx2, ty2 = ocr_item['bbox']\n\n        ix1 = max(sx1, tx1)\n        iy1 = max(sy1, ty1)\n        ix2 = min(sx2, tx2)\n        iy2 = min(sy2, ty2)\n\n        if ix1 &lt; ix2 and iy1 &lt; iy2:\n            intersection = (ix2 - ix1) * (iy2 - iy1)\n            text_area = (tx2 - tx1) * (ty2 - ty1)\n\n            overlap_ratio = intersection / text_area if text_area &gt; 0 else 0\n\n            if overlap_ratio &gt; threshold and overlap_ratio &gt; best_overlap:\n                best_overlap = overlap_ratio\n                best_rotation = ocr_item.get('polygon_rotation',\n                                            ocr_item.get('paddleocr_orientation', 0))\n\n    return best_rotation\n\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nfrom sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\ndef check_overlap(box1, box2):\n    x1_a, y1_a, x2_a, y2_a = box1\n    x1_b, y1_b, x2_b, y2_b = box2\n    if x2_a &lt; x1_b or x2_b &lt; x1_a:\n        return False\n    if y2_a &lt; y1_b or y2_b &lt; y1_a:\n        return False\n    return True\ndetections = detections_list\ninclude_labels = set([\n    \"Pentagon\", \"Racetrack\",\n    \"Triangle\", \"arrow\", \"rectangle\"\n])\nfiltered_detections = [det for det in detections if det[0] in include_labels]\nboxes = [[float(x1), float(y1), float(x2), float(y2)] for _, (x1, y1), (x2, y2) in filtered_detections]\nos.chdir(\"/content/sam2/sam2/configs/sam2\")\nmodel_cfg = \"sam2_hiera_b+.yaml\"\nsam2_checkpoint = \"/content/sam2/checkpoints/sam2_hiera_base_plus.pt\"\nsam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cpu\")\npredictor = SAM2ImagePredictor(sam2_model)\nimage = Image.open(file_path).convert(\"RGB\")\ncompletable_shapes = {'circle', 'rounded rectangle', 'rectangle', 'Racetrack'}\nocclusion_flags = []\nfor i in range(len(filtered_detections)):\n    is_occluded = False\n    label_i = filtered_detections[i][0]\n    box_i = boxes[i]\n    if label_i in completable_shapes:\n        for j in range(len(filtered_detections)):\n            if i == j: continue\n            box_j = boxes[j]\n            if check_overlap(box_i, box_j):\n                is_occluded = True\n                break\n    occlusion_flags.append(is_occluded)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nangle_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n])\nmodel_configs = {\n    'rectangle': {\"path\": \"/content/angle-models-weights/final_rectangle.pth\", \"num_classes\": 4},\n    'Triangle': {\"path\": \"/content/angle-models-weights/Triangle.pth\", \"num_classes\": 4},\n    'Racetrack': {\"path\": \"/content/angle-models-weights/final_racetrack.pth\", \"num_classes\": 4},\n    'Pentagon': {\"path\": \"/content/angle-models-weights/best_resnet18_pentagon.pth\", \"num_classes\": 2},\n    'arrow': {\"path\": \"/content/angle-models-weights/best_resnet18_arrow.pth\", \"num_classes\": 4},\n}\nangle_models = {}\nfor shape_name, config in model_configs.items():\n    if os.path.exists(config[\"path\"]):\n        model = models.resnet18(pretrained=False)\n        model.fc = nn.Linear(model.fc.in_features, config[\"num_classes\"])\n        model.load_state_dict(torch.load(config[\"path\"], map_location=device))\n        model = model.to(device)\n        model.eval()\n        angle_models[shape_name] = model\n        print(f\"Loaded angle model for {shape_name} from {config['path']}\")\n    else:\n        print(f\"Warning: Angle model for {shape_name} not found at {config['path']}\")\ntriangle_aux_model_path = \"/content/angle-models-weights/Triangle_Angle_90_270.pth\"\nif os.path.exists(triangle_aux_model_path):\n    triangle_aux_model = models.resnet18(pretrained=False)\n    triangle_aux_model.fc = nn.Linear(triangle_aux_model.fc.in_features, 2)\n    triangle_aux_model.load_state_dict(torch.load(triangle_aux_model_path, map_location=device))\n    triangle_aux_model = triangle_aux_model.to(device)\n    triangle_aux_model.eval()\n    angle_models[\"triangle_aux\"] = triangle_aux_model\ndef predict_angle(pil_image, model):\n    img_tensor = angle_transform(pil_image).unsqueeze(0).to(device)\n    with torch.no_grad():\n        outputs = model(img_tensor)\n        _, pred = torch.max(outputs, 1)\n    return pred.item()\nwith torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n    predictor.set_image(image)\n    masks, iou_predictions, _ = predictor.predict(box=boxes)\nprint(f\"Processed {len(boxes)} bounding boxes.\")\nprint(f\"Received {len(masks)} sets of mask candidates.\")\n\npredicted_angles = []\nfor i in range(len(boxes)):\n    label = filtered_detections[i][0]\n    is_occluded = occlusion_flags[i]\n    original_bbox = boxes[i]\n    box_masks = masks[i]\n    box_scores = iou_predictions[i]\n    best_mask_index = torch.argmax(torch.from_numpy(box_scores))\n    best_mask = box_masks[best_mask_index]\n    mask_binary = (best_mask &gt; 0).astype(np.uint8)\n    contours, _ = cv2.findContours(mask_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    if contours:\n        largest_contour = max(contours, key=cv2.contourArea)\n        processed_mask = np.zeros_like(mask_binary)\n        if label in completable_shapes and is_occluded:\n            print(f\"Applying occlusion correction to '{label}' (Box {i}).\")\n            if label == 'circle':\n                (x, y), radius = cv2.minEnclosingCircle(largest_contour)\n                cv2.circle(processed_mask, (int(x), int(y)), int(radius), (1), thickness=cv2.FILLED)\n            elif label == 'Racetrack':\n                rect = cv2.minAreaRect(largest_contour)\n                center, (width, height), angle = rect\n                if width &lt; height:\n                    width, height = height, width\n                    angle += 90\n                radius = height / 2\n                if width &lt;= height:\n                    cv2.circle(processed_mask, (int(center[0]), int(center[1])), int(radius), (1), thickness=cv2.FILLED)\n                else:\n                    rect_center = np.array(center)\n                    angle_rad = np.deg2rad(angle)\n                    half_rect_len = width / 2 - radius\n                    dx = np.cos(angle_rad) * half_rect_len\n                    dy = np.sin(angle_rad) * half_rect_len\n                    p1 = rect_center + np.array([dx, dy])\n                    p2 = rect_center - np.array([dx, dy])\n                    cv2.circle(processed_mask, (int(p1[0]), int(p1[1])), int(radius), (1), thickness=cv2.FILLED)\n                    cv2.circle(processed_mask, (int(p2[0]), int(p2[1])), int(radius), (1), thickness=cv2.FILLED)\n                    perp_dx = -np.sin(angle_rad) * radius\n                    perp_dy = np.cos(angle_rad) * radius\n                    corner1 = p1 + np.array([perp_dx, perp_dy])\n                    corner2 = p1 - np.array([perp_dx, perp_dy])\n                    corner3 = p2 - np.array([perp_dx, perp_dy])\n                    corner4 = p2 + np.array([perp_dx, perp_dy])\n                    rect_contour = np.array([corner1, corner2, corner3, corner4], dtype=np.intp)\n                    cv2.drawContours(processed_mask, [rect_contour], 0, (1), thickness=cv2.FILLED)\n            else:\n                hull = cv2.convexHull(largest_contour)\n                cv2.drawContours(processed_mask, [hull], -1, (1), thickness=cv2.FILLED)\n        else:\n            cv2.drawContours(processed_mask, [largest_contour], -1, (1), thickness=cv2.FILLED)\n        mask_binary = processed_mask\n\n    rows = np.any(mask_binary, axis=1)\n    cols = np.any(mask_binary, axis=0)\n    if not rows.any() or not cols.any():\n        print(f\"Mask for box {i} is empty after processing, skipping.\")\n        continue\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n    padding_percentage = 0.1\n    height_pad = rmax - rmin\n    width_pad = cmax - cmin\n    padding_y = int(height_pad * padding_percentage)\n    padding_x = int(width_pad * padding_percentage)\n    padded_rmin = rmin - padding_y\n    padded_rmax = rmax + padding_y\n    padded_cmin = cmin - padding_x\n    padded_cmax = cmax + padding_x\n    full_mask_height, full_mask_width = mask_binary.shape\n    padded_rmin = max(0, padded_rmin)\n    padded_rmax = min(full_mask_height - 1, padded_rmax)\n    padded_cmin = max(0, padded_cmin)\n    padded_cmax = min(full_mask_width - 1, padded_cmax)\n    padded_crop = mask_binary[padded_rmin:padded_rmax+1, padded_cmin:padded_cmax+1]\n    final_image_rgb = np.zeros((padded_crop.shape[0], padded_crop.shape[1], 3), dtype=np.uint8)\n    random_color = np.random.randint(0, 256, size=3)\n    for c in range(3):\n        final_image_rgb[:, :, c][padded_crop == 1] = random_color[c]\n    cropped_pil_image = Image.fromarray(final_image_rgb)\n\n    predicted_angle_info = \"N/A\"\n    angle_source = \"none\"\n\n\n    x1, y1, x2, y2 = original_bbox\n    shape_bbox = (int(x1), int(y1), int(x2), int(y2))\n\n    if 'final_processed_data' in dir() and final_processed_data:\n        text_rotation = get_text_rotation_for_shape(shape_bbox, final_processed_data)\n        if text_rotation is not None:\n            normalized_rotation = text_rotation % 360\n            predicted_angle_info = f\"{normalized_rotation:.1f} \"\n            angle_source = \"ocr_text\"\n            print(f\"Using OCR text rotation for '{label}' (Box {i}): {predicted_angle_info}\")\n\n\n    if angle_source == \"none\" and label in angle_models:\n        angle_model = angle_models[label]\n        num_classes = model_configs[label][\"num_classes\"]\n        angle_class = predict_angle(cropped_pil_image, angle_model)\n        angle_source = \"resnet_model\"\n\n        if label == 'Pentagon':\n            if angle_class == 0:\n                predicted_angle_info = \"0 \"\n            elif angle_class == 1:\n                predicted_angle_info = \"180 \"\n            else:\n                predicted_angle_info = f\"Pentagon Class {angle_class}\"\n        elif label == 'rectangle':\n            x1, y1, x2, y2 = original_bbox\n            width = x2 - x1\n            height = y2 - y1\n            if abs(width - height) &lt; 5:\n                predicted_angle_info = \"0 \"\n            elif height &gt;= 2 * width:\n                predicted_angle_info = \"90 \"\n            else:\n                predicted_angle_info = \"0 \"\n        elif label == 'Racetrack':\n            racetrack_angles = {0: \"45 \", 1: \"90 \", 2: \"135 \", 3: \"0 \"}\n            predicted_angle_info = racetrack_angles.get(angle_class, f\"Racetrack Class {angle_class}\")\n        elif label == 'Triangle':\n            if angle_class == 0:\n                predicted_angle_info = \"0 \"\n            elif angle_class == 2:\n                predicted_angle_info = \"180 \"\n            else:\n                if \"triangle_aux\" in angle_models:\n                    aux_class = predict_angle(cropped_pil_image, angle_models[\"triangle_aux\"])\n                    if aux_class == 0:\n                        predicted_angle_info = \"90  (rightward tip)\"\n                    elif aux_class == 1:\n                        predicted_angle_info = \"270  (leftward tip)\"\n                    else:\n                        predicted_angle_info = f\"Triangle Auxiliary Class {aux_class} (unknown direction)\"\n                else:\n                    predicted_angle_info = \"Triangle auxiliary weights not found.\"\n        elif label in ['arrow']:\n            arrow_angles = {0: \"0 \", 1: \"90 \", 2: \"180 \", 3: \"270 \"}\n            predicted_angle_info = arrow_angles.get(angle_class, f\"{label.capitalize()} Class {angle_class}\")\n        else:\n            if num_classes == 8:\n                angle_class_to_degrees_8_classes = {\n                    0: \"0 \", 1: \"45 \", 2: \"90 \", 3: \"135 \",\n                    4: \"180 \", 5: \"225 \", 6: \"270 \", 7: \"315 \"\n                }\n                predicted_angle_info = angle_class_to_degrees_8_classes.get(angle_class, f\"Class {angle_class} (Unknown Degree)\")\n            elif num_classes == 4:\n                generic_4_class_angles = {\n                    0: \"0 \", 1: \"90 \", 2: \"180 \", 3: \"270 \"\n                }\n                predicted_angle_info = generic_4_class_angles.get(angle_class, f\"Class {angle_class} (Unknown Degree)\")\n            elif num_classes == 2:\n                generic_2_class_angles = {\n                    0: \"0 \", 1: \"180 \"\n                }\n                predicted_angle_info = generic_2_class_angles.get(angle_class, f\"Class {angle_class} (Unknown Degree)\")\n            else:\n                predicted_angle_info = f\"Class {angle_class}\"\n    elif angle_source == \"none\":\n        print(f\"No angle model available for '{label}' (Box {i}). Skipping angle prediction.\")\n\n    predicted_angles.append({\n        \"angle\": predicted_angle_info,\n        \"source\": angle_source\n    })\n    print(f\"Predicted Angle for '{label}' (Box {i}): {predicted_angle_info} (source: {angle_source})\")\n\nUserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\nUserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n\n\nLoaded angle model for rectangle from /content/angle-models-weights/final_rectangle.pth\nLoaded angle model for Triangle from /content/angle-models-weights/Triangle.pth\nLoaded angle model for Racetrack from /content/angle-models-weights/final_racetrack.pth\nLoaded angle model for Pentagon from /content/angle-models-weights/best_resnet18_pentagon.pth\nLoaded angle model for arrow from /content/angle-models-weights/best_resnet18_arrow.pth\nProcessed 6 bounding boxes.\nReceived 6 sets of mask candidates.\nApplying occlusion correction to 'Racetrack' (Box 0).\nUsing OCR text rotation for 'Racetrack' (Box 0): 249.2 \nPredicted Angle for 'Racetrack' (Box 0): 249.2  (source: ocr_text)\nUsing OCR text rotation for 'Pentagon' (Box 1): 0.0 \nPredicted Angle for 'Pentagon' (Box 1): 0.0  (source: ocr_text)\nApplying occlusion correction to 'Racetrack' (Box 2).\nUsing OCR text rotation for 'Racetrack' (Box 2): 319.0 \nPredicted Angle for 'Racetrack' (Box 2): 319.0  (source: ocr_text)\nApplying occlusion correction to 'Racetrack' (Box 3).\nUsing OCR text rotation for 'Racetrack' (Box 3): 0.0 \nPredicted Angle for 'Racetrack' (Box 3): 0.0  (source: ocr_text)\nApplying occlusion correction to 'Racetrack' (Box 4).\nUsing OCR text rotation for 'Racetrack' (Box 4): 357.4 \nPredicted Angle for 'Racetrack' (Box 4): 357.4  (source: ocr_text)\nApplying occlusion correction to 'Racetrack' (Box 5).\nUsing OCR text rotation for 'Racetrack' (Box 5): 1.9 \nPredicted Angle for 'Racetrack' (Box 5): 1.9  (source: ocr_text)\n\n\n\n\nDominant Color Prediction\n\nimport cv2\nimport numpy as np\n\ndef get_corner_background_color(image):\n\n    h, w = image.shape[:2]\n    s = min(5, w // 2, h // 2)\n\n    tl = image[0:s, 0:s].reshape(-1, 3)\n    tr = image[0:s, w-s:w].reshape(-1, 3)\n    bl = image[h-s:h, 0:s].reshape(-1, 3)\n    br = image[h-s:h, w-s:w].reshape(-1, 3)\n\n    corners = np.vstack([tl, tr, bl, br])\n    mean_bgr = np.mean(corners, axis=0)\n    return mean_bgr\n\ndef find_smart_dominant_color(image, bg_color_bgr, k=4):\n\n    try:\n        lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n        pixels = lab_image.reshape((-1, 3)).astype(np.float32)\n\n        if pixels.shape[0] &lt; 10:\n            return None\n\n        K = min(k, max(1, pixels.shape[0] // 200))\n        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n        _, labels, centers = cv2.kmeans(\n            pixels, K, None, criteria, 10, cv2.KMEANS_PP_CENTERS\n        )\n\n        counts = np.bincount(labels.flatten())\n\n        bg_color_uint8 = np.array([[bg_color_bgr]], dtype=np.uint8)\n        bg_lab = cv2.cvtColor(bg_color_uint8, cv2.COLOR_BGR2LAB)[0][0].astype(np.float32)\n\n        sorted_indices = np.argsort(counts)[::-1]\n\n        final_color_bgr = None\n\n        for idx in sorted_indices:\n            center_lab = centers[idx]\n\n            dist = np.linalg.norm(center_lab - bg_lab)\n\n            if dist &gt; 15:\n                color_lab = center_lab.astype(\"uint8\").reshape(1, 1, 3)\n                final_color_bgr = cv2.cvtColor(color_lab, cv2.COLOR_LAB2BGR).reshape(3)\n                break\n\n        if final_color_bgr is None:\n            color_lab = centers[sorted_indices[0]].astype(\"uint8\").reshape(1, 1, 3)\n            final_color_bgr = cv2.cvtColor(color_lab, cv2.COLOR_LAB2BGR).reshape(3)\n\n        return tuple(int(c) for c in final_color_bgr[::-1])\n    except Exception as e:\n        return None\n\ndef rgb_to_hex(rgb_tuple):\n    if rgb_tuple is None:\n        return None\n    r, g, b = rgb_tuple\n    return \"#{:02x}{:02x}{:02x}\".format(\n        max(0, min(255, int(r))),\n        max(0, min(255, int(g))),\n        max(0, min(255, int(b)))\n    )\n\ndef iou(boxA, boxB):\n    ax1,ay1,ax2,ay2 = boxA\n    bx1,by1,bx2,by2 = boxB\n    inter_x1 = max(ax1,bx1); inter_y1 = max(ay1,by1)\n    inter_x2 = min(ax2,bx2); inter_y2 = min(ay2,by2)\n    inter_w = max(0, inter_x2 - inter_x1); inter_h = max(0, inter_y2 - inter_y1)\n    inter_area = inter_w * inter_h\n    areaA = max(0, ax2-ax1) * max(0, ay2-ay1)\n    areaB = max(0, bx2-bx1) * max(0, by2-by1)\n    denom = areaA + areaB - inter_area\n    return inter_area/denom if denom &gt; 0 else 0.0\n\ndef greedy_iou_match(detections, angle_source, predicted_angles, iou_threshold=0.4):\n    if angle_source is None or predicted_angles is None:\n        return {}\n\n    S = min(len(angle_source), len(predicted_angles))\n    if S == 0:\n        return {}\n\n    D = len(detections)\n    det_labels, det_boxes = [], []\n    for d in detections:\n        det_labels.append(d[0])\n        p1,p2 = d[1], d[2]\n        det_boxes.append((int(round(p1[0])), int(round(p1[1])), int(round(p2[0])), int(round(p2[1]))))\n\n    src_labels, src_boxes = [], []\n    for j in range(S):\n        s = angle_source[j]\n        src_labels.append(s[0])\n        sp1, sp2 = s[1], s[2]\n        src_boxes.append((int(round(sp1[0])), int(round(sp1[1])), int(round(sp2[0])), int(round(sp2[1]))))\n\n    iou_mat = np.zeros((D, S), dtype=float)\n    for i in range(D):\n        for j in range(S):\n            if det_labels[i] != src_labels[j]:\n                continue\n            iou_mat[i, j] = iou(det_boxes[i], src_boxes[j])\n\n    matches = {}\n    used_det = set(); used_src = set()\n    while True:\n        if iou_mat.size == 0:\n            break\n        idx = np.unravel_index(np.argmax(iou_mat), iou_mat.shape)\n        max_iou = iou_mat[idx]\n        if max_iou &lt; iou_threshold:\n            break\n        di, sj = int(idx[0]), int(idx[1])\n        if di in used_det or sj in used_src:\n            iou_mat[di, sj] = 0.0\n            continue\n        matches[di] = predicted_angles[sj]\n        used_det.add(di); used_src.add(sj)\n        iou_mat[di, :] = 0.0\n        iou_mat[:, sj] = 0.0\n\n    return matches\n\ndef process_image_and_detections_simple(image_path, detections, predicted_angles=None, angle_source=None, iou_threshold=0.4):\n    img = cv2.imread(image_path)\n    if img is None:\n        raise FileNotFoundError(f\"Image not found at path: {image_path}\")\n    H, W = img.shape[:2]\n\n    thin_line_labels = {\"dashed-arrow\", \"doted-arrow\", \"connector\"}\n\n    allowed_angle_labels = {\"arrow\", \"rectangle\", \"Racetrack\", \"Pentagon\", \"Triangle\"}\n\n    matches = greedy_iou_match(detections, angle_source, predicted_angles, iou_threshold=iou_threshold)\n\n    out = []\n    for i, det in enumerate(detections):\n        if len(det) &gt;= 4:\n            label, p1, p2, existing_color = det[0], det[1], det[2], det[3]\n        else:\n            label, p1, p2 = det[0], det[1], det[2]\n            existing_color = None\n\n        x1 = max(0, int(round(float(p1[0])))); y1 = max(0, int(round(float(p1[1]))))\n        x2 = min(W, int(round(float(p2[0])))); y2 = min(H, int(round(float(p2[1]))))\n\n        if x2 &lt;= x1 or y2 &lt;= y1:\n            dom_hex = existing_color\n        else:\n            full_crop = img[y1:y2, x1:x2]\n\n            if full_crop is None or full_crop.size == 0:\n                dom_hex = existing_color\n            else:\n\n                bg_color_bgr = get_corner_background_color(full_crop)\n\n                h_c, w_c = full_crop.shape[:2]\n                margin_h = int(h_c * 0.20)\n                margin_w = int(w_c * 0.20)\n\n                if margin_h &gt; 0 and margin_w &gt; 0:\n                    center_crop = full_crop[margin_h:h_c-margin_h, margin_w:w_c-margin_w]\n                else:\n                    center_crop = full_crop\n\n                dom_rgb = find_smart_dominant_color(center_crop, bg_color_bgr)\n                dom_hex = existing_color if existing_color else rgb_to_hex(dom_rgb)\n\n        angle_val = None\n        if label in allowed_angle_labels:\n            if i in matches:\n                angle_val = matches[i]\n            elif predicted_angles is not None and i &lt; len(predicted_angles):\n                angle_val = predicted_angles[i]\n\n        out.append((label, (x1, y1), (x2, y2), dom_hex, angle_val))\n\n    return out\n\ntry:\n    detections_input = detections\nexcept NameError:\n    try:\n        detections_input = detections_list\n    except NameError:\n        raise RuntimeError(\"Set 'detections' or 'detections_list' before running this cell.\")\n\npreds = globals().get('predicted_angles', None)\nsrc = globals().get('filtered_detections', None)\n\nfinal_detections = process_image_and_detections_simple(file_path, detections_input, predicted_angles=preds, angle_source=src, iou_threshold=0.4)\n\n\nprint(\"detections = [\")\nfor det in final_detections:\n    label, (x1, y1), (x2, y2), color_hex, angle_info = det\n    print(f\"    ('{label}', ({x1}, {y1}), ({x2}, {y2}), {repr(color_hex)}, {repr(angle_info)}),\")\nprint(\"]\")\n\ndetections = [\n    ('Racetrack', (525, 337), (773, 732), '#5f803e', {'angle': '249.2 ', 'source': 'ocr_text'}),\n    ('circle', (75, 354), (211, 486), '#ba6c26', None),\n    ('Pentagon', (198, 193), (629, 608), '#263619', {'angle': '0.0 ', 'source': 'ocr_text'}),\n    ('Racetrack', (399, 37), (734, 357), '#faf9e7', {'angle': '319.0 ', 'source': 'ocr_text'}),\n    ('circle', (191, 625), (328, 758), '#ba6c26', None),\n    ('circle', (540, 567), (676, 700), '#ba6c26', None),\n    ('Racetrack', (91, 35), (442, 352), '#6d7a4b', {'angle': '0.0 ', 'source': 'ocr_text'}),\n    ('circle', (580, 198), (715, 331), '#ba6c26', None),\n    ('circle', (290, 64), (423, 196), '#ba6c26', None),\n    ('Racetrack', (182, 610), (573, 773), '#dcaa73', {'angle': '357.4 ', 'source': 'ocr_text'}),\n    ('Racetrack', (58, 338), (307, 737), '#c8945a', {'angle': '1.9 ', 'source': 'ocr_text'}),\n]\n\n\n\n\nFinal JSON Output\n\nimport numpy as np\nimport json\nfrom PIL import Image\n\ndef create_nodes_json(shape_detections):\n\n    diagram_nodes = []\n    for i, det in enumerate(shape_detections):\n        label = det[0] if len(det) &gt; 0 else \"\"\n        top_left = det[1] if len(det) &gt; 1 else (0, 0)\n        bottom_right = det[2] if len(det) &gt; 2 else (0, 0)\n        color = det[3] if len(det) &gt; 3 else None\n        angle = det[4] if len(det) &gt; 4 else None\n\n        if angle is None:\n            angle_val = \"\"\n        elif isinstance(angle, dict):\n            raw = angle.get(\"angle\", \"\")\n            if isinstance(raw, str):\n                angle_val = raw.strip()\n            else:\n                angle_val = str(raw) if raw is not None else \"\"\n        elif isinstance(angle, str):\n            angle_val = angle.strip()\n            if angle_val.lower() == \"none\":\n                angle_val = \"\"\n        else:\n            # It's a number\n            angle_val = str(angle)\n\n        x1, y1 = top_left\n        x2, y2 = bottom_right\n        center_x = int(round((x1 + x2) / 2))\n        center_y = int(round((y1 + y2) / 2))\n        width = int(round(x2 - x1))\n        height = int(round(y2 - y1))\n\n        node = {\n            \"id\": f\"node{i+1}\",\n            \"x\": center_x,\n            \"y\": center_y,\n            \"text\": \"\",\n            \"shape\": label,\n            \"color\": color,\n            \"width\": width,\n            \"height\": height,\n            \"angle\": angle_val\n        }\n        diagram_nodes.append(node)\n    return diagram_nodes\n\ndef _det_to_center(det):\n    if det is None:\n        return None\n    label = det[0] if len(det) &gt; 0 else \"\"\n    tl = det[1] if len(det) &gt; 1 else (0, 0)\n    br = det[2] if len(det) &gt; 2 else (0, 0)\n    cx = int(round((tl[0] + br[0]) / 2))\n    cy = int(round((tl[1] + br[1]) / 2))\n    return (label, cx, cy)\n\ndef _find_node_by_det(diagram_nodes, det, tol=8):\n    if det is None:\n        return None\n    label, cx, cy = _det_to_center(det)\n    for node in diagram_nodes:\n        if node['shape'] == label:\n            if abs(node['x'] - cx) &lt;= tol and abs(node['y'] - cy) &lt;= tol:\n                return node['id']\n    best = (None, 1e9)\n    for node in diagram_nodes:\n        if node['shape'] == label:\n            dist = (node['x']-cx)**2 + (node['y']-cy)**2\n            if dist &lt; best[1]:\n                best = (node['id'], dist)\n    return best[0]\n\ndef create_edges_json(arrow_connections, diagram_nodes, all_detections, arrowhead_radius=50):\n    diagram_edges = []\n    arrow_head_centers = []\n    for d in [dd for dd in all_detections if dd[0] == \"arrow_head\"]:\n        tl = d[1] if len(d) &gt; 1 else (0, 0)\n        br = d[2] if len(d) &gt; 2 else (0, 0)\n        arrow_head_centers.append(((tl[0]+br[0])/2.0, (tl[1]+br[1])/2.0))\n\n    for i, conn in enumerate(arrow_connections):\n        tail_det = conn.get(\"tail_connected_to\")\n        head_det = conn.get(\"head_connected_to\")\n        src_id = _find_node_by_det(diagram_nodes, tail_det)\n        tgt_id = _find_node_by_det(diagram_nodes, head_det)\n        if not src_id or not tgt_id:\n            continue\n\n        head_pt = tuple(conn.get('head')) if conn.get('head') is not None else None\n        tail_pt = tuple(conn.get('tail')) if conn.get('tail') is not None else None\n\n        def _has_arrow_at(pt):\n            if pt is None:\n                return False\n            for ah in arrow_head_centers:\n                if np.linalg.norm(np.array(pt) - np.array(ah)) &lt; arrowhead_radius:\n                    return True\n            return False\n\n        has_at_tail = _has_arrow_at(tail_pt)\n        has_at_head = _has_arrow_at(head_pt)\n\n        startArrow = False\n        endArrow = False\n\n        if has_at_tail and has_at_head:\n            startArrow = True; endArrow = True\n        elif has_at_tail and not has_at_head:\n            src_id, tgt_id = tgt_id, src_id; startArrow = False; endArrow = True\n        elif has_at_head and not has_at_tail:\n            startArrow = False; endArrow = True\n        else:\n            if head_pt and tail_pt:\n                x_diff = head_pt[0] - tail_pt[0]\n                y_diff = head_pt[1] - tail_pt[1]\n                if abs(x_diff) &gt; abs(y_diff):\n                    if x_diff &lt; 0:\n                        src_id, tgt_id = tgt_id, src_id\n                else:\n                    if y_diff &lt; 0:\n                        src_id, tgt_id = tgt_id, src_id\n\n        raw_label = conn.get(\"original_label\") or conn.get(\"label\") or \"\"\n        line_style = raw_label.split('-')[0] if raw_label else \"solid\"\n\n        edge = {\n            \"id\": f\"edge{i+1}\",\n            \"source\": src_id,\n            \"target\": tgt_id,\n            \"lineStyle\": line_style,\n            \"startArrow\": bool(startArrow),\n            \"endArrow\": bool(endArrow),\n            \"color\": \"#333333\"\n        }\n        diagram_edges.append(edge)\n\n    return diagram_edges\n\ndef map_ocr_to_nodes(diagram_nodes, processed_ocr_data, relaxation_pixels=15, max_area_ratio=80):\n\n    nodes = [n.copy() for n in diagram_nodes]\n    ocr_boxes = []\n    if not processed_ocr_data:\n        return nodes, []\n\n    for ocr in processed_ocr_data:\n        bbox = ocr.get('bbox')\n        text = ocr.get('text', \"\")\n        if not bbox or not text.strip():\n            continue\n\n        ocr_x1, ocr_y1, ocr_x2, ocr_y2 = bbox\n        ocr_w = max(1, ocr_x2 - ocr_x1)\n        ocr_h = max(1, ocr_y2 - ocr_y1)\n        ocr_area = ocr_w * ocr_h\n\n        candidates = []\n        for idx, node in enumerate(nodes):\n            cx, cy, w, h = node['x'], node['y'], node['width'], node['height']\n            node_x1 = cx - (w // 2) - relaxation_pixels\n            node_y1 = cy - (h // 2) - relaxation_pixels\n            node_x2 = cx + (w // 2) + relaxation_pixels\n            node_y2 = cy + (h // 2) + relaxation_pixels\n\n            if (ocr_x1 &gt;= node_x1 and ocr_y1 &gt;= node_y1 and ocr_x2 &lt;= node_x2 and ocr_y2 &lt;= node_y2):\n                node_area = max(1, w * h)\n                area_ratio = node_area / float(ocr_area)\n                dx = cx - (ocr_x1 + ocr_x2) / 2.0\n                dy = cy - (ocr_y1 + ocr_y2) / 2.0\n                center_dist2 = dx * dx + dy * dy\n                candidates.append((idx, node_area, area_ratio, center_dist2))\n\n        if candidates:\n            filtered = [c for c in candidates if c[2] &lt;= max_area_ratio]\n            chosen = min(filtered if filtered else candidates, key=lambda t: (t[1], t[3]))\n            best_idx = chosen[0]\n            node = nodes[best_idx]\n            node['text'] = (node.get('text', \"\") + (\" \" if node.get('text') else \"\") + text).strip()\n        else:\n            ocr_boxes.append({\n                \"text\": text,\n                \"x1\": int(ocr_x1), \"y1\": int(ocr_y1),\n                \"x2\": int(ocr_x2), \"y2\": int(ocr_y2)\n            })\n\n    standalone_text_labels = []\n    merge_y_thresh = 18\n    merge_x_thresh = 40\n    if ocr_boxes:\n        ocr_boxes = sorted(ocr_boxes, key=lambda b: (b['y1'], b['x1']))\n        used = [False] * len(ocr_boxes)\n        for i, box in enumerate(ocr_boxes):\n            if used[i]:\n                continue\n            texts = [box['text'].strip()]\n            x1, y1, x2, y2 = box['x1'], box['y1'], box['x2'], box['y2']\n            used[i] = True\n            for j in range(i + 1, len(ocr_boxes)):\n                if used[j]:\n                    continue\n                ob = ocr_boxes[j]\n                ob_x1, ob_y1, ob_x2, ob_y2 = ob['x1'], ob['y1'], ob['x2'], ob['y2']\n                vertical_close = abs(ob_y1 - y2) &lt;= merge_y_thresh or abs(ob_y2 - y1) &lt;= merge_y_thresh\n                horizontal_overlap = (ob_x1 &lt;= x2 + merge_x_thresh and ob_x2 &gt;= x1 - merge_x_thresh)\n                if vertical_close and horizontal_overlap:\n                    texts.append(ob['text'].strip())\n                    x1 = min(x1, ob_x1)\n                    y1 = min(y1, ob_y1)\n                    x2 = max(x2, ob_x2)\n                    y2 = max(y2, ob_y2)\n                    used[j] = True\n\n            standalone_text_labels.append({\n                \"id\": f\"text{len(standalone_text_labels) + 1}\",\n                \"x\": int(round((x1 + x2) / 2)),\n                \"y\": int(round((y1 + y2) / 2)),\n                \"text\": \" \".join(texts).strip(),\n                \"bbox\": {\"x1\": x1, \"y1\": y1, \"x2\": x2, \"y2\": y2},\n                \"width\": x2 - x1,\n                \"height\": y2 - y1\n            })\n\n    return nodes, standalone_text_labels\n\n\n# ---- Build final JSON ----\nsource_detections = None\nif 'final_detections' in globals() and isinstance(globals()['final_detections'], list):\n    source_detections = globals()['final_detections']\nelif 'final_detections' in globals() and globals()['final_detections'] is None:\n    source_detections = None\nelif 'detections' in globals() and isinstance(globals()['detections'], list):\n    source_detections = globals()['detections']\nelif 'detections_list' in globals() and isinstance(globals()['detections_list'], list):\n    source_detections = globals()['detections_list']\nelse:\n    raise RuntimeError(\"No detections available. Ensure `final_detections` or `detections` is defined in scope.\")\n\nshape_detections = [d for d in source_detections if d[0] not in [\"dashed-arrow\", \"dotted-arrow\", \"solid-arrow\", \"arrow_head\"]]\nnodes_json = create_nodes_json(shape_detections)\n\nif 'arrow_connections' not in globals():\n    arrow_connections = []\nedges_json = create_edges_json(arrow_connections, nodes_json, source_detections)\n\nstandalone_text_labels = []\nif 'final_processed_data' in globals() and final_processed_data:\n    nodes_json, standalone_text_labels = map_ocr_to_nodes(\n        nodes_json, final_processed_data, relaxation_pixels=15,\n)\n\nif 'image' not in globals():\n    raise RuntimeError(\"`image` not found in scope. Provide `image` (PIL Image or numpy array).\")\nimg_obj = globals()['image']\nif hasattr(img_obj, \"shape\"):\n    canvas_w, canvas_h = int(img_obj.shape[1]), int(img_obj.shape[0])\nelse:\n    try:\n        canvas_w, canvas_h = int(img_obj.size[0]), int(img_obj.size[1])\n    except Exception as e:\n        raise RuntimeError(\"Cannot determine canvas size from `image`. Provide a numpy array or PIL Image.\") from e\n\nfinal_json_output = {\n    \"canvas\": {\"width\": canvas_w, \"height\": canvas_h},\n    \"nodes\": nodes_json,\n    \"edges\": edges_json,\n    \"text_labels\": standalone_text_labels\n}\n\nprint(\"\\n--- Final JSON Output ---\")\nprint(json.dumps(final_json_output, indent=2))\n\n\n--- Final JSON Output ---\n{\n  \"canvas\": {\n    \"width\": 801,\n    \"height\": 798\n  },\n  \"nodes\": [\n    {\n      \"id\": \"node1\",\n      \"x\": 649,\n      \"y\": 534,\n      \"text\": \"Hope is a dangerous illusion\",\n      \"shape\": \"Racetrack\",\n      \"color\": \"#5f803e\",\n      \"width\": 248,\n      \"height\": 395,\n      \"angle\": \"249.2\"\n    },\n    {\n      \"id\": \"node2\",\n      \"x\": 143,\n      \"y\": 420,\n      \"text\": \"#5\",\n      \"shape\": \"circle\",\n      \"color\": \"#ba6c26\",\n      \"width\": 136,\n      \"height\": 132,\n      \"angle\": \"\"\n    },\n    {\n      \"id\": \"node3\",\n      \"x\": 414,\n      \"y\": 400,\n      \"text\": \"Fragments of Fire: Short Truths for a Heavy Mind\",\n      \"shape\": \"Pentagon\",\n      \"color\": \"#263619\",\n      \"width\": 431,\n      \"height\": 415,\n      \"angle\": \"0.0\"\n    },\n    {\n      \"id\": \"node4\",\n      \"x\": 566,\n      \"y\": 197,\n      \"text\": \"The void always stares back.\",\n      \"shape\": \"Racetrack\",\n      \"color\": \"#faf9e7\",\n      \"width\": 335,\n      \"height\": 320,\n      \"angle\": \"319.0\"\n    },\n    {\n      \"id\": \"node5\",\n      \"x\": 260,\n      \"y\": 692,\n      \"text\": \"#4\",\n      \"shape\": \"circle\",\n      \"color\": \"#ba6c26\",\n      \"width\": 137,\n      \"height\": 133,\n      \"angle\": \"\"\n    },\n    {\n      \"id\": \"node6\",\n      \"x\": 608,\n      \"y\": 634,\n      \"text\": \"#3\",\n      \"shape\": \"circle\",\n      \"color\": \"#ba6c26\",\n      \"width\": 136,\n      \"height\": 133,\n      \"angle\": \"\"\n    },\n    {\n      \"id\": \"node7\",\n      \"x\": 266,\n      \"y\": 194,\n      \"text\": \"We rot beautifully.\",\n      \"shape\": \"Racetrack\",\n      \"color\": \"#6d7a4b\",\n      \"width\": 351,\n      \"height\": 317,\n      \"angle\": \"0.0\"\n    },\n    {\n      \"id\": \"node8\",\n      \"x\": 648,\n      \"y\": 264,\n      \"text\": \"#2\",\n      \"shape\": \"circle\",\n      \"color\": \"#ba6c26\",\n      \"width\": 135,\n      \"height\": 133,\n      \"angle\": \"\"\n    },\n    {\n      \"id\": \"node9\",\n      \"x\": 356,\n      \"y\": 130,\n      \"text\": \"#1\",\n      \"shape\": \"circle\",\n      \"color\": \"#ba6c26\",\n      \"width\": 133,\n      \"height\": 132,\n      \"angle\": \"\"\n    },\n    {\n      \"id\": \"node10\",\n      \"x\": 378,\n      \"y\": 692,\n      \"text\": \"Light lies; shadows remember.\",\n      \"shape\": \"Racetrack\",\n      \"color\": \"#dcaa73\",\n      \"width\": 391,\n      \"height\": 163,\n      \"angle\": \"357.4\"\n    },\n    {\n      \"id\": \"node11\",\n      \"x\": 182,\n      \"y\": 538,\n      \"text\": \"Especially you. Everything lends.\",\n      \"shape\": \"Racetrack\",\n      \"color\": \"#c8945a\",\n      \"width\": 249,\n      \"height\": 399,\n      \"angle\": \"1.9\"\n    }\n  ],\n  \"edges\": [],\n  \"text_labels\": []\n}\n\n\n\n\nMAP\n# This is formatted as code\nMean Average Precision of the RF-DETR model\n\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom PIL import Image\nfrom supervision.metrics.mean_average_precision import MeanAveragePrecision\n\nimages_directory_path = \"/content/Dataset/valid\"\nannotations_path = \"/content/Dataset/valid/_annotations.coco.json\"\npreTrainedWeights = \"/content/weights/checkpoint_best_regular.pth\"\n\nmodel = RFDETRMedium(pretrain_weights=preTrainedWeights)\nmodel.optimize_for_inference()\n\ntest_dataset = sv.DetectionDataset.from_coco(\n    images_directory_path=images_directory_path,\n    annotations_path=annotations_path,\n)\n\n\ndef model_callback(image: Image.Image) -&gt; sv.Detections:\n    return model.predict(image)\n\nsticky_note_class_name = \"Sticky Notes\"\nsticky_note_class_idx = None\nfor idx, cls_name in enumerate(test_dataset.classes):\n    if cls_name.lower() == sticky_note_class_name.lower():\n        sticky_note_class_idx = idx\n        break\n\nmap_metric = MeanAveragePrecision()\nall_predictions = []\nall_ground_truths = []\nfor path, image, ground_truth in test_dataset:\n    preds = model_callback(image)\n    if sticky_note_class_idx is not None:\n        preds = preds[preds.class_id != sticky_note_class_idx]\n        ground_truth = ground_truth[ground_truth.class_id != sticky_note_class_idx]\n    all_predictions.append(preds)\n    all_ground_truths.append(ground_truth)\n\nmap_metric.update(all_predictions, all_ground_truths)\n\nresult = map_metric.compute()\n\nprint(f\"Overall mAP@0.5:0.95 = {result.map50_95:.4f}\")\nprint(f\"mAP@0.5 = {result.map50:.4f}\")\nprint(\"Number of classes in dataset:\", len(test_dataset.classes))\nprint(\"Number of classes with AP computed:\", result.ap_per_class.shape[0])\n\nprint(\"Per-class AP:\")\nfor i, class_idx in enumerate(result.matched_classes):\n    if sticky_note_class_idx is not None and class_idx == sticky_note_class_idx:\n        continue\n    class_name = test_dataset.classes[class_idx]\n    ap_scores = result.ap_per_class[i]\n    mean_ap = ap_scores.mean()\n    print(f\"{class_name}: {mean_ap:.4f}\")\n\nUsing a different number of positional encodings than DINOv2, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.\nUsing patch size 16 instead of 14, which means we're not loading DINOv2 backbone weights. This is not a problem if finetuning a pretrained RF-DETR model.\nLoading pretrain weights\n\n\nWARNING:rfdetr.main:num_classes mismatch: pretrain weights has 16 classes, but your model has 90 classes\nreinitializing detection head with 16 classes\n`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: torch.as_tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\nTracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\nTracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\nTracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\nUserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4322.)\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\nTracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\nTracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).\n\n\nOverall mAP@0.5:0.95 = 0.8265\nmAP@0.5 = 0.9478\nNumber of classes in dataset: 17\nNumber of classes with AP computed: 15\nPer-class AP:\nCloud: 0.8402\nDiamond: 0.8372\nDouble Arrow: 0.9713\nPentagon: 0.9066\nRacetrack: 0.9020\nStar: 0.9215\nTriangle: 0.9800\narrow: 0.9016\narrow_head: 0.5326\ncircle: 0.8608\ndashed-arrow: 0.6638\ndotted-arrow: 0.6027\nrectangle: 0.8934\nrounded rectangle: 0.8837\nsolid-arrow: 0.6995\n\n\nVerifying the Arrow connections for finding the algorithm accuracy\n\nimport json\nimport matplotlib.pyplot as plt\n\n\njson_data = final_json_output\n\n# --- Parse JSON ---\ndata = json_data\nnodes = {node[\"id\"]: node for node in data[\"nodes\"]}\nedges = data[\"edges\"]\n# --- Setup Canvas ---\nfig, ax = plt.subplots(figsize=(data[\"canvas\"][\"width\"]/100, data[\"canvas\"][\"height\"]/100))\nax.set_xlim(0, data[\"canvas\"][\"width\"])\nax.set_ylim(0, data[\"canvas\"][\"height\"])\nax.invert_yaxis()\nax.axis(\"off\")\n\nfor node in nodes.values():\n    label = node[\"text\"] if node[\"text\"] else node[\"shape\"]\n    ax.text(node[\"x\"], node[\"y\"], label, ha=\"center\", va=\"center\", fontsize=8, color=\"black\", wrap=True)\nfor edge in edges:\n    src = nodes[edge[\"source\"]]\n    tgt = nodes[edge[\"target\"]]\n    x1, y1 = src[\"x\"], src[\"y\"]\n    x2, y2 = tgt[\"x\"], tgt[\"y\"]\n    ax.annotate(\"\",\n                xy=(x2, y2), xytext=(x1, y1),\n                arrowprops=dict(arrowstyle=\"-&gt;\", color=\"gray\", lw=1.2))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n!pip install pandas\n\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\nRequirement already satisfied: numpy&gt;=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas) (1.17.0)\n\n\nConfusion metrix on the Output of the algorithm of arrow connections\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef clean_and_extract_number(value):\n\n    if pd.isna(value):\n        return 0\n    s_value = str(value).strip()\n    parts = s_value.split(' ', 1)\n    numeric_part = parts[0]\n\n    try:\n        return int(float(numeric_part))\n    except (ValueError, TypeError):\n        return 0\n\ndef create_confusion_matrix(file_path):\n\n    try:\n        df = pd.read_excel(file_path)\n        df.columns = df.columns.str.strip()\n\n        numeric_columns = ['TOTAL ARROW', 'CORECT ARROW NUMBER', 'WRONG ARROW NUMBER']\n        for col in numeric_columns:\n            if col in df.columns:\n                df[col] = df[col].apply(clean_and_extract_number)\n            else:\n                print(f\"Warning: Column '{col}' not found. Please check spelling in your Excel file.\")\n\n        true_positives = df['CORECT ARROW NUMBER'].sum()\n        false_positives = df['WRONG ARROW NUMBER'].sum()\n        false_negatives = (df['TOTAL ARROW'] - df['CORECT ARROW NUMBER']).sum()\n\n        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) &gt; 0 else 0\n        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) &gt; 0 else 0\n\n        print(\"----------------------------------------\")\n        print(\"   Algorithm Performance Metrics\")\n        print(\"----------------------------------------\")\n        print(f\"True Positives (TP):  {true_positives}\")\n        print(f\"False Positives (FP): {false_positives}\")\n        print(f\"False Negatives (FN): {false_negatives}\")\n        print(\"True Negatives (TN):  Zero 0.\")\n        print(\"\\n--- Key Metrics ---\")\n        print(f\"Precision: {precision:.2%}\")\n        print(f\"Recall (Sensitivity): {recall:.2%}\")\n        print(\"----------------------------------------\\n\")\n\n        matrix_data = [[true_positives, false_negatives], [false_positives, 0]]\n\n        matrix_labels = [\n            [f\"{true_positives}\\n(TP)\", f\"{false_negatives}\\n(FN)\"],\n            [f\"{false_positives}\\n(FP)\", \"Zero (0) \\n(TN)\"]\n        ]\n\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(matrix_data, annot=matrix_labels, fmt=\"\", cmap='Blues',\n                    xticklabels=['Predicted: Arrow', 'Predicted: Not Arrow'],\n                    yticklabels=['Actual: Arrow', 'Actual: Not Arrow'],\n                    cbar=False, annot_kws={\"size\": 14})\n\n        plt.xlabel('Predicted Label', fontsize=12)\n        plt.ylabel('True Label', fontsize=12)\n        plt.title('Confusion Matrix for Arrow Detection', fontsize=16)\n\n        # This command displays the plot\n        plt.show()\n\n    except FileNotFoundError:\n        print(f\"Error: The file '{file_path}' was not found.\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\nif __name__ == '__main__':\n    file_path = '/content/REPORT.xlsx'\n    create_confusion_matrix(file_path)\n\n----------------------------------------\n   Algorithm Performance Metrics\n----------------------------------------\nTrue Positives (TP):  294\nFalse Positives (FP): 10\nFalse Negatives (FN): 10\nTrue Negatives (TN):  Zero 0.\n\n--- Key Metrics ---\nPrecision: 96.71%\nRecall (Sensitivity): 96.71%\n----------------------------------------\n\n\n\n\n\n\n\n\n\n\ncorrect\n\nimport json\nimport math\nimport textwrap\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw, ImageFont\n\nclass DiagramGenerator:\n    def __init__(self, json_data):\n        self.data = json_data\n        self.canvas_w = json_data['canvas']['width']\n        self.canvas_h = json_data['canvas']['height']\n        self.image = Image.new('RGB', (self.canvas_w, self.canvas_h), color='white')\n        self.draw = ImageDraw.Draw(self.image)\n\n        try:\n            self.font = ImageFont.truetype(\"arial.ttf\", 14)\n            self.font_bold = ImageFont.truetype(\"arialbd.ttf\", 13)\n        except IOError:\n            self.font = ImageFont.load_default()\n            self.font_bold = ImageFont.load_default()\n\n    def generate_image(self):\n        nodes = self.data.get('nodes', [])\n\n\n        sorted_nodes = sorted(nodes, key=lambda n: n['width'] * n['height'], reverse=True)\n\n        for node in sorted_nodes:\n            self.draw_node(node)\n\n        for label in self.data.get('text_labels', []):\n            self.draw_label(label)\n\n        return self.image\n\n    def get_bbox(self, node):\n        cx, cy = node['x'], node['y']\n        w, h = node['width'], node['height']\n        return (cx - w/2, cy - h/2, cx + w/2, cy + h/2)\n\n    def get_poly_points(self, cx, cy, w, h, shape_type):\n        if shape_type == \"diamond\":\n            return [(0, -h/2), (w/2, 0), (0, h/2), (-w/2, 0)]\n        elif shape_type == \"triangle\":\n            return [(0, -h/2), (w/2, h/2), (-w/2, h/2)]\n        elif shape_type == \"pentagon\":\n            return self._calculate_ngon(5, w/2, h/2)\n        elif \"arrow\" in shape_type and \"double\" not in shape_type:\n            head_len = w * 0.4\n            return [(-w/2, -h/4), (w/2 - head_len, -h/4), (w/2 - head_len, -h/2),\n                    (w/2, 0), (w/2 - head_len, h/2), (w/2 - head_len, h/4), (-w/2, h/4)]\n        elif \"double arrow\" in shape_type:\n            head_w = w * 0.2\n            shaft_h = h * 0.4\n            return [(-w/2, 0), (-w/2 + head_w, -h/2), (-w/2 + head_w, -shaft_h/2),\n                    (w/2 - head_w, -shaft_h/2), (w/2 - head_w, -h/2), (w/2, 0),\n                    (w/2 - head_w, h/2), (w/2 - head_w, shaft_h/2),\n                    (-w/2 + head_w, shaft_h/2), (-w/2 + head_w, h/2)]\n        return [(-w/2, -h/2), (w/2, -h/2), (w/2, h/2), (-w/2, h/2)]\n\n    def draw_node(self, node):\n        raw_shape = node.get('shape', 'rectangle')\n        shape_type = raw_shape.lower().replace(\"_\", \" \").strip()\n\n        cx, cy = node['x'], node['y']\n        w, h = node['width'], node['height']\n\n        color = node.get('color', '#cccccc')\n        if len(color) == 9 and color.startswith('#'): color = color[:7]\n\n        try: angle = float(node.get('angle', 0))\n        except: angle = 0\n\n        text = node.get('text', '')\n\n        x1, y1, x2, y2 = self.get_bbox(node)\n\n        if shape_type in [\"circle\", \"ellipse\", \"start\", \"end\"]:\n            self.draw.ellipse([x1, y1, x2, y2], fill=color)\n        elif shape_type == \"racetrack\" or shape_type == \"terminal\":\n            self.draw.rounded_rectangle([x1, y1, x2, y2], radius=h/2, fill=color)\n        elif shape_type in [\"rounded rectangle\", \"process\"]:\n            self.draw.rounded_rectangle([x1, y1, x2, y2], radius=15, fill=color)\n        elif shape_type == \"cloud\":\n            self.draw.ellipse([x1, y1, x2, y2], fill=color)\n        elif shape_type == \"star\":\n            points = self._calculate_star_points(5, w/2, w/4)\n            self._draw_rotated_polygon(points, cx, cy, angle, color)\n        else:\n            points = self.get_poly_points(cx, cy, w, h, shape_type)\n            self._draw_rotated_polygon(points, cx, cy, angle, color)\n\n        if text:\n            self._draw_text_centered(text, cx, cy, w, h)\n\n    def draw_label(self, label):\n        self.draw.text((label['x'], label['y']), label.get('text', ''), font=self.font, fill=\"black\")\n\n    def _rotate_point(self, point, angle_deg):\n        angle_rad = math.radians(angle_deg)\n        x, y = point\n        return (x * math.cos(angle_rad) - y * math.sin(angle_rad),\n                x * math.sin(angle_rad) + y * math.cos(angle_rad))\n\n    def _draw_rotated_polygon(self, points, cx, cy, angle, color):\n        rotated = []\n        for p in points:\n            rx, ry = self._rotate_point(p, angle)\n            rotated.append((cx + rx, cy + ry))\n        self.draw.polygon(rotated, fill=color)\n\n    def _calculate_ngon(self, sides, radius_x, radius_y):\n        points = []\n        for i in range(sides):\n            angle = (2 * math.pi * i) / sides - (math.pi / 2)\n            points.append((math.cos(angle) * radius_x, math.sin(angle) * radius_y))\n        return points\n\n    def _calculate_star_points(self, points_count, outer_radius, inner_radius):\n        points = []\n        angle_step = math.pi / points_count\n        current_angle = -math.pi / 2\n        for i in range(points_count * 2):\n            radius = outer_radius if i % 2 == 0 else inner_radius\n            points.append((math.cos(current_angle) * radius, math.sin(current_angle) * radius))\n            current_angle += angle_step\n        return points\n\n    def _draw_text_centered(self, text, cx, cy, w, h):\n        if not text: return\n\n        char_w_approx = 6.5\n        padding = 10\n        max_chars = max(1, int((w - padding) / char_w_approx))\n\n        lines = textwrap.wrap(text, width=max_chars)\n\n        line_height = 14\n        total_text_h = len(lines) * line_height\n\n        current_y = cy - (total_text_h / 2)\n\n        for line in lines:\n            bbox = self.draw.textbbox((0, 0), line, font=self.font_bold)\n            text_w = bbox[2] - bbox[0]\n            text_x = cx - (text_w / 2)\n            self.draw.text((text_x, current_y), line, font=self.font_bold, fill=\"black\")\n            current_y += line_height\n\nif __name__ == \"__main__\":\n\n    data = {\n  \"canvas\": {\n    \"width\": 801,\n    \"height\": 798\n  },\n  \"nodes\": [\n    {\n      \"id\": \"node1\",\n      \"x\": 649,\n      \"y\": 534,\n      \"text\": \"Hope is a dangerous illusion\",\n      \"shape\": \"Racetrack\",\n      \"color\": \"#5f803e\",\n      \"width\": 248,\n      \"height\": 395,\n      \"angle\": {\n        \"angle\": \"290.7 \",\n        \"source\": \"ocr_text\"\n      }\n    },\n    {\n      \"id\": \"node2\",\n      \"x\": 143,\n      \"y\": 420,\n      \"text\": \"#5\",\n      \"shape\": \"circle\",\n      \"color\": \"#ba6c26\",\n      \"width\": 136,\n      \"height\": 132,\n      \"angle\": \"\"\n    },\n    {\n      \"id\": \"node3\",\n      \"x\": 414,\n      \"y\": 400,\n      \"text\": \"Fragments of Fire: Short Truths for a Heavy Mind\",\n      \"shape\": \"Pentagon\",\n      \"color\": \"#263619\",\n      \"width\": 431,\n      \"height\": 415,\n      \"angle\": {\n        \"angle\": \"0.0 \",\n        \"source\": \"ocr_text\"\n      }\n    },\n    {\n      \"id\": \"node4\",\n      \"x\": 566,\n      \"y\": 197,\n      \"text\": \"The void always stares back.\",\n      \"shape\": \"Racetrack\",\n      \"color\": \"#faf9e7\",\n      \"width\": 335,\n      \"height\": 320,\n      \"angle\": {\n        \"angle\": \"41.2 \",\n        \"source\": \"ocr_text\"\n      }\n    },\n    {\n      \"id\": \"node5\",\n      \"x\": 260,\n      \"y\": 692,\n      \"text\": \"#4\",\n      \"shape\": \"circle\",\n      \"color\": \"#ba6c26\",\n      \"width\": 137,\n      \"height\": 133,\n      \"angle\": \"\"\n    },\n    {\n      \"id\": \"node6\",\n      \"x\": 608,\n      \"y\": 634,\n      \"text\": \"#3\",\n      \"shape\": \"circle\",\n      \"color\": \"#ba6c26\",\n      \"width\": 136,\n      \"height\": 133,\n      \"angle\": \"\"\n    },\n    {\n      \"id\": \"node7\",\n      \"x\": 266,\n      \"y\": 194,\n      \"text\": \"We rot beautifully.\",\n      \"shape\": \"Racetrack\",\n      \"color\": \"#6d7a4b\",\n      \"width\": 351,\n      \"height\": 317,\n      \"angle\": {\n        \"angle\": \"0.0 \",\n        \"source\": \"ocr_text\"\n      }\n    },\n    {\n      \"id\": \"node8\",\n      \"x\": 648,\n      \"y\": 264,\n      \"text\": \"#2\",\n      \"shape\": \"circle\",\n      \"color\": \"#ba6c26\",\n      \"width\": 135,\n      \"height\": 133,\n      \"angle\": \"\"\n    },\n    {\n      \"id\": \"node9\",\n      \"x\": 356,\n      \"y\": 130,\n      \"text\": \"#1\",\n      \"shape\": \"circle\",\n      \"color\": \"#ba6c26\",\n      \"width\": 133,\n      \"height\": 132,\n      \"angle\": \"\"\n    },\n    {\n      \"id\": \"node10\",\n      \"x\": 378,\n      \"y\": 692,\n      \"text\": \"Light lies; shadows remember.\",\n      \"shape\": \"Racetrack\",\n      \"color\": \"#dcaa73\",\n      \"width\": 391,\n      \"height\": 163,\n      \"angle\": {\n        \"angle\": \"2.5 \",\n        \"source\": \"ocr_text\"\n      }\n    },\n    {\n      \"id\": \"node11\",\n      \"x\": 182,\n      \"y\": 538,\n      \"text\": \"Especially you. Everything lends.\",\n      \"shape\": \"Racetrack\",\n      \"color\": \"#c8945a\",\n      \"width\": 249,\n      \"height\": 399,\n      \"angle\": {\n        \"angle\": \"178.0 \",\n        \"source\": \"ocr_text\"\n      }\n    }\n  ],\n  \"edges\": [],\n  \"text_labels\": []\n}\n\n    gen = DiagramGenerator(data)\n    final_img = gen.generate_image()\n\n    plt.figure(figsize=(10, 8))\n    plt.imshow(final_img)\n    plt.axis('off')\n    plt.show()",
    "crumbs": [
      "**Downloading the Dataset and the weights of the pre trained model**"
    ]
  },
  {
    "objectID": "spatial_pipeline.html",
    "href": "spatial_pipeline.html",
    "title": "Spatial Pipeline",
    "section": "",
    "text": "source\n\nrun_spatial_mapping\n\n run_spatial_mapping (file_path:str)",
    "crumbs": [
      "Spatial Pipeline"
    ]
  },
  {
    "objectID": "rf_detr.html",
    "href": "rf_detr.html",
    "title": "Type for detection: (label, top_left, bottom_right, confidence)",
    "section": "",
    "text": "source\n\ndetect_shapes\n\n detect_shapes (file_path:str, threshold:float=0.25,\n                raise_on_model_failure:bool=False)\n\n*Run RF-DETR inference and return list of (label, (x1,y1), (x2,y2), confidence).\nReturns empty list if model or image cannot be loaded, unless raise_on_model_failure is True.*",
    "crumbs": [
      "Type for detection: (label, top_left, bottom_right, confidence)"
    ]
  },
  {
    "objectID": "node_builder.html",
    "href": "node_builder.html",
    "title": "Node Builder",
    "section": "",
    "text": "source\n\nattach_angles_to_nodes\n\n attach_angles_to_nodes (diagram_nodes, detections, angle_map)\n\n\nsource\n\n\nfind_node_by_det\n\n find_node_by_det (diagram_nodes, det, tol=8)\n\n\nsource\n\n\ncreate_nodes_json\n\n create_nodes_json (shape_detections_with_color)",
    "crumbs": [
      "Node Builder"
    ]
  },
  {
    "objectID": "arrow_detector.html",
    "href": "arrow_detector.html",
    "title": "Arrow Detector",
    "section": "",
    "text": "source\n\nfind_arrow_endpoints\n\n find_arrow_endpoints (image_bgr, detections, ocr_results)\n\n\nsource\n\n\nconvert_to_binary_mask\n\n convert_to_binary_mask (cropped_img)\n\n\nsource\n\n\ncrop_image_region\n\n crop_image_region (image, top_left, bottom_right)",
    "crumbs": [
      "Arrow Detector"
    ]
  },
  {
    "objectID": "skeleton_analyzer.html",
    "href": "skeleton_analyzer.html",
    "title": "Skeleton Analyzer",
    "section": "",
    "text": "source\n\nfind_best_arrow_by_straightness\n\n find_best_arrow_by_straightness (binary_mask, min_path_length=20)\n\n\nsource\n\n\nbfs_get_path\n\n bfs_get_path (skeleton, start_xy, end_xy)\n\n\nsource\n\n\nget_skeleton_graph_nodes\n\n get_skeleton_graph_nodes (skeleton)\n\n\nsource\n\n\nextract_skeleton\n\n extract_skeleton (binary_mask)\n\n\nsource\n\n\nskeleton_has_cycle\n\n skeleton_has_cycle (skeleton)",
    "crumbs": [
      "Skeleton Analyzer"
    ]
  },
  {
    "objectID": "angle_predictor.html",
    "href": "angle_predictor.html",
    "title": "Angle Predictor",
    "section": "",
    "text": "# import os\n# from typing import List, Dict, Any\n# import numpy as np\n\n# _TORCH_AVAILABLE = True\n# try:\n#     import torch\n#     from torchvision import models, transforms\n#     from PIL import Image\n# except Exception:\n#     _TORCH_AVAILABLE = False\n\n# _ANGLE_ALLOWED = {\"rectangle\", \"Triangle\", \"Racetrack\", \"Pentagon\", \"arrow\"}\n# _ANGLE_MODEL_PATHS = {\n#     \"rectangle\": os.getenv(\"ANGLE_RECTANGLE\", \"weights/angle-models/final_rectangle.pth\"),\n#     \"Triangle\": os.getenv(\"ANGLE_TRIANGLE\", \"weights/angle-models/Triangle.pth\"),\n#     \"Racetrack\": os.getenv(\"ANGLE_RACETRACK\", \"weights/angle-models/final_racetrack.pth\"),\n#     \"Pentagon\": os.getenv(\"ANGLE_PENTAGON\", \"weights/angle-models/best_resnet18_pentagon.pth\"),\n#     \"arrow\": os.getenv(\"ANGLE_ARROW\", \"weights/angle-models/best_resnet18_arrow.pth\"),\n# }\n# _ANGLE_NUM_CLASSES = {\n#     \"rectangle\": 4,\n#     \"Triangle\": 4,\n#     \"Racetrack\": 4,\n#     \"Pentagon\": 2,\n#     \"arrow\": 4,\n# }\n# _ANGLE_MODELS = None  \n# _ANGLE_TRANSFORM = None\n\n# def _angle_label_for(label: str, idx: int) -&gt; str:\n#     try:\n#         i = int(idx)\n#     except Exception:\n#         return \"\"\n#     if label == \"Pentagon\":\n#         return \"0 degrees\" if i == 0 else (\"180 degrees\" if i == 1 else \"\")\n#     mapping4 = {0: \"0 degrees\", 1: \"90 degrees\", 2: \"180 degrees\", 3: \"270 degrees\"}\n#     return mapping4.get(i, \"\")\n\n# def _get_angle_transform():\n#     global _ANGLE_TRANSFORM\n#     if _ANGLE_TRANSFORM is not None:\n#         return _ANGLE_TRANSFORM\n#     if not _TORCH_AVAILABLE:\n#         return None\n#     _ANGLE_TRANSFORM = transforms.Compose([\n#         transforms.Resize((224, 224)),\n#         transforms.ToTensor(),\n#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n#     ])\n#     return _ANGLE_TRANSFORM\n\n# def _load_angle_models():\n#     global _ANGLE_MODELS\n#     if _ANGLE_MODELS is not None:\n#         return _ANGLE_MODELS\n#     if not _TORCH_AVAILABLE:\n#         _ANGLE_MODELS = {}\n#         return _ANGLE_MODELS\n#     models_dict = {}\n#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n#     for label, path in _ANGLE_MODEL_PATHS.items():\n#         num_classes = _ANGLE_NUM_CLASSES.get(label)\n#         if not num_classes:\n#             continue\n#         if not os.path.exists(path):\n#             continue\n#         try:\n#             m = models.resnet18(pretrained=False)\n#             m.fc = torch.nn.Linear(m.fc.in_features, num_classes)\n#             state = torch.load(path, map_location=device)\n#             if isinstance(state, dict) and \"state_dict\" in state:\n#                 state = state[\"state_dict\"]\n#             try:\n#                 m.load_state_dict(state, strict=False)\n#             except Exception:\n#                 m.load_state_dict(state, strict=False)\n#             m.to(device)\n#             m.eval()\n#             models_dict[label] = (m, device)\n#         except Exception:\n#             continue\n#     _ANGLE_MODELS = models_dict\n#     return _ANGLE_MODELS\n\n# def _crop_to_pil(image_bgr, tl, br):\n#     import cv2\n#     x1, y1 = map(int, map(round, tl))\n#     x2, y2 = map(int, map(round, br))\n#     h, w = image_bgr.shape[:2]\n#     x1, y1 = max(0, x1), max(0, y1)\n#     x2, y2 = min(w, x2), min(h, y2)\n#     if x2 &lt;= x1 or y2 &lt;= y1:\n#         return None\n#     crop = image_bgr[y1:y2, x1:x2]\n#     if crop.size == 0:\n#         return None\n#     rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n#     return Image.fromarray(rgb)\n\n# def _predict_angle_for_det(image_bgr, label: str, tl, br, models_dict: Dict[str, Any]) -&gt; str:\n#     if label not in _ANGLE_ALLOWED:\n#         return \"\"\n#     if not models_dict or label not in models_dict:\n#         return \"\"\n#     transform = _get_angle_transform()\n#     if transform is None:\n#         return \"\"\n#     pil_img = _crop_to_pil(image_bgr, tl, br)\n#     if pil_img is None:\n#         return \"\"\n#     tensor = transform(pil_img).unsqueeze(0)\n#     model, device = models_dict[label]\n#     try:\n#         with torch.no_grad():\n#             logits = model(tensor.to(device))\n#             idx = int(torch.argmax(logits, dim=1).item())\n#         return _angle_label_for(label, idx) or \"\"\n#     except Exception:\n#         return \"\"\n\n# def predict_angles(image_bgr, detections):\n#     models_dict = _load_angle_models()\n#     if not models_dict:\n#         return {}\n#     angle_map = {}\n#     for det in detections:\n#         try:\n#             label, tl, br = det\n#         except Exception:\n#             continue\n#         angle = _predict_angle_for_det(image_bgr, label, tl, br, models_dict)\n#         angle_map[det] = angle if isinstance(angle, str) else \"\"\n#     return angle_map\n\n\n\n\n\n#| export\nimport os\nimport logging\nfrom PIL import Image\n\ntry:\n    import torch\n    import torch.nn as nn\n    from torchvision import models, transforms\nexcept Exception:\n    torch = None\n    nn = None\n    models = None\n    transforms = None\n\nlog = logging.getLogger(\"angle_detection\")\n\n# Backwards-compatible constants expected by sam2_processor\n_TORCH_AVAILABLE = True if torch is not None else False\n\n# Angle configuration mirrors notebook\n_ANGLE_ALLOWED = {\"rectangle\", \"Triangle\", \"Racetrack\", \"Pentagon\", \"arrow\"}\n_ANGLE_MODEL_PATHS = {\n    \"rectangle\": os.getenv(\"ANGLE_RECTANGLE\", \"weights/angle-models/final_rectangle.pth\"),\n    \"Triangle\": os.getenv(\"ANGLE_TRIANGLE\", \"weights/angle-models/Triangle.pth\"),\n    \"Racetrack\": os.getenv(\"ANGLE_RACETRACK\", \"weights/angle-models/final_racetrack.pth\"),\n    \"Pentagon\": os.getenv(\"ANGLE_PENTAGON\", \"weights/angle-models/best_resnet18_pentagon.pth\"),\n    \"arrow\": os.getenv(\"ANGLE_ARROW\", \"weights/angle-models/best_resnet18_arrow.pth\"),\n}\n_ANGLE_NUM_CLASSES = {\n    \"rectangle\": 4,\n    \"Triangle\": 4,\n    \"Racetrack\": 4,\n    \"Pentagon\": 2,\n    \"arrow\": 4,\n}\n\n_ANGLE_MODELS = None\n_ANGLE_TRANSFORM = None\n\n\ndef _angle_label_for(label: str, idx: int) -&gt; str:\n    try:\n        i = int(idx)\n    except Exception:\n        return \"\"\n    if label == \"Pentagon\":\n        return \"0 degrees\" if i == 0 else (\"180 degrees\" if i == 1 else \"\")\n    mapping4 = {0: \"0 degrees\", 1: \"90 degrees\", 2: \"180 degrees\", 3: \"270 degrees\"}\n    return mapping4.get(i, \"\")\n\n\ndef _get_angle_transform():\n    global _ANGLE_TRANSFORM\n    if _ANGLE_TRANSFORM is not None:\n        return _ANGLE_TRANSFORM\n    if not _TORCH_AVAILABLE:\n        return None\n    _ANGLE_TRANSFORM = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n    ])\n    return _ANGLE_TRANSFORM\n\n\ndef _load_angle_models():\n    global _ANGLE_MODELS\n    if _ANGLE_MODELS is not None:\n        return _ANGLE_MODELS\n    if not _TORCH_AVAILABLE:\n        _ANGLE_MODELS = {}\n        return _ANGLE_MODELS\n    models_dict = {}\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    for label, path in _ANGLE_MODEL_PATHS.items():\n        num_classes = _ANGLE_NUM_CLASSES.get(label)\n        if not num_classes:\n            continue\n        if not os.path.exists(path):\n            continue\n        try:\n            m = models.resnet18(pretrained=False)\n            m.fc = torch.nn.Linear(m.fc.in_features, num_classes)\n            state = torch.load(path, map_location=device)\n            if isinstance(state, dict) and \"state_dict\" in state:\n                state = state[\"state_dict\"]\n            try:\n                m.load_state_dict(state, strict=False)\n            except Exception:\n                m.load_state_dict(state, strict=False)\n            m.to(device)\n            m.eval()\n            models_dict[label] = (m, device)\n        except Exception:\n            continue\n    _ANGLE_MODELS = models_dict\n    return _ANGLE_MODELS\n\n\ndef _crop_to_pil(image_bgr, tl, br):\n    import cv2\n    x1, y1 = map(int, map(round, tl))\n    x2, y2 = map(int, map(round, br))\n    h, w = image_bgr.shape[:2]\n    x1, y1 = max(0, x1), max(0, y1)\n    x2, y2 = min(w, x2), min(h, y2)\n    if x2 &lt;= x1 or y2 &lt;= y1:\n        return None\n    crop = image_bgr[y1:y2, x1:x2]\n    if crop.size == 0:\n        return None\n    rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n    return Image.fromarray(rgb)\n\n\ndef _predict_angle_for_det(image_bgr, label: str, tl, br, models_dict: dict) -&gt; str:\n    if label not in _ANGLE_ALLOWED:\n        return \"\"\n    if not models_dict or label not in models_dict:\n        return \"\"\n    transform = _get_angle_transform()\n    if transform is None:\n        return \"\"\n    pil_img = _crop_to_pil(image_bgr, tl, br)\n    if pil_img is None:\n        return \"\"\n    tensor = transform(pil_img).unsqueeze(0)\n    model, device = models_dict[label]\n    try:\n        with torch.no_grad():\n            logits = model(tensor.to(device))\n            idx = int(torch.argmax(logits, dim=1).item())\n        return _angle_label_for(label, idx) or \"\"\n    except Exception:\n        return \"\"\n\n\ndef predict_angles(image_bgr, detections):\n    models_dict = _load_angle_models()\n    if not models_dict:\n        return {}\n    angle_map = {}\n    for det in detections:\n        try:\n            label, tl, br = det[0], det[1], det[2]\n        except Exception:\n            continue\n        angle = _predict_angle_for_det(image_bgr, label, tl, br, models_dict)\n        angle_map[det] = angle if isinstance(angle, str) else \"\"\n    return angle_map\n\n\nsource\n\npredict_angles\n\n predict_angles (image_bgr, detections)",
    "crumbs": [
      "Angle Predictor"
    ]
  },
  {
    "objectID": "geometry_utils.html",
    "href": "geometry_utils.html",
    "title": "Geometry Utils",
    "section": "",
    "text": "source\n\nestablish_connections\n\n establish_connections (vectors, detections)\n\n\nsource\n\n\nfind_farthest_points_euclidean\n\n find_farthest_points_euclidean (binary_mask)\n\n\nsource\n\n\nfind_farthest_points_in_contour\n\n find_farthest_points_in_contour (contour)\n\n\nsource\n\n\nfind_nearest_shape_bbox\n\n find_nearest_shape_bbox (point, shapes, max_distance=None)\n\n\nsource\n\n\nnearest_point_on_bbox\n\n nearest_point_on_bbox (point, top_left, bottom_right)\n\n\nsource\n\n\ncheck_bbox_intersection\n\n check_bbox_intersection (box1_tl, box1_br, box2_tl, box2_br)",
    "crumbs": [
      "Geometry Utils"
    ]
  },
  {
    "objectID": "main.html",
    "href": "main.html",
    "title": "AI Board Scanner – End‑to‑End Demo",
    "section": "",
    "text": "# Example usage (update to a real image path on your system)\ndemo_image = \"path/to/your/test_image.png\"\n\n# This will run the complete pipeline and display the structured result\nresult = run_demo(demo_image)\nresult",
    "crumbs": [
      "AI Board Scanner – End‑to‑End Demo"
    ]
  },
  {
    "objectID": "11_paddle_ocr.html",
    "href": "11_paddle_ocr.html",
    "title": "nbdev_testing_simon2",
    "section": "",
    "text": "source\n\nextract_text\n\n extract_text (file_path:str)\n\nRun PaddleOCR and return list of {text, confidence, bbox(x1,y1,x2,y2)}\n\nsource\n\n\nsnap_to_standard_angle\n\n snap_to_standard_angle (angle:float, tolerance:int=20)\n\nSnap angle to nearest standard angle if within tolerance.\n\nsource\n\n\ncalculate_rotation_from_polygon\n\n calculate_rotation_from_polygon (polygon)\n\nCalculate rotation angle from text polygon using PCA/SVD.",
    "crumbs": [
      "11_paddle_ocr.html"
    ]
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "Models",
    "section": "",
    "text": "source\n\nSpatialMapResponse\n\n SpatialMapResponse (success:bool=True, data:__main__.SpatialData,\n                     metadata:__main__.ProcessingMetadata=&lt;factory&gt;)\n\nResponse containing the spatial mapping result with metadata\n\nsource\n\n\nSpatialData\n\n SpatialData (canvas:__main__.Canvas, nodes:List[__main__.Node]=&lt;factory&gt;,\n              edges:List[__main__.Edge]=&lt;factory&gt;,\n              text_labels:List[__main__.TextLabel]=&lt;factory&gt;)\n\nComplete spatial mapping data\n\nsource\n\n\nProcessingMetadata\n\n ProcessingMetadata (timestamp:datetime.datetime=&lt;factory&gt;,\n                     processing_time_ms:Annotated[Optional[int],Ge(ge=0)]=\n                     None, api_version:str='1.0.0',\n                     warnings:List[str]=&lt;factory&gt;)\n\nMetadata about the processing operation\n\nsource\n\n\nEdge\n\n Edge (id:Annotated[str,_PydanticGeneralMetadata(pattern='^edge\\\\d+$')],\n       source:Annotated[str,_PydanticGeneralMetadata(pattern='^node\\\\d+$')\n       ], target:Annotated[str,_PydanticGeneralMetadata(pattern='^node\\\\d+\n       $')], lineStyle:Literal['solid','dashed','dotted']='solid',\n       startArrow:bool=False, endArrow:bool=True, color:Annotated[str,_Pyd\n       anticGeneralMetadata(pattern='^#[0-9a-fA-F]{6}$')]='#333333',\n       label:Optional[str]=None,\n       points:Optional[List[__main__.Point]]=None,\n       confidence:Annotated[Optional[float],Ge(ge=0.0),Le(le=1.0)]=None)\n\nAn edge represents a connection between two nodes\n\nsource\n\n\nPoint\n\n Point (x:float, y:float)\n\nA 2D point coordinate\n\nsource\n\n\nNode\n\n Node (id:Annotated[str,_PydanticGeneralMetadata(pattern='^node\\\\d+$')],\n       x:Annotated[int,Ge(ge=0)], y:Annotated[int,Ge(ge=0)], text:str='', \n       shape:Literal['cloud','diamond','double_arrow','pentagon','racetrac\n       k','star','triangle','arrow','circle','rectangle','rounded_rectangl\n       e'], color:Annotated[str,_PydanticGeneralMetadata(pattern='^#[0-9a-\n       fA-F]{6}$')], angle:Annotated[int,Ge(ge=0),Le(le=359)]=0,\n       width:Annotated[int,Gt(gt=0)], height:Annotated[int,Gt(gt=0)],\n       bbox:Optional[__main__.BBox]=None,\n       confidence:Annotated[Optional[float],Ge(ge=0.0),Le(le=1.0)]=None)\n\nA node represents a shape detected in the image\n\nsource\n\n\nCanvas\n\n Canvas (width:int, height:int)\n\nCanvas dimensions\n\nsource\n\n\nSpatialMapRequest\n\n SpatialMapRequest (file_path:str)\n\nRequest model for spatial map processing.\n\nsource\n\n\nTextLabel\n\n TextLabel (id:str, x:int, y:int, text:str, bbox:__main__.BBox, width:int,\n            height:int)\n\nA standalone text label not contained within a shape\n\nsource\n\n\nBBox\n\n BBox (x1:Annotated[int,Ge(ge=0)], y1:Annotated[int,Ge(ge=0)],\n       x2:Annotated[int,Ge(ge=0)], y2:Annotated[int,Ge(ge=0)])\n\nBounding box coordinates\n\nsource\n\n\nShapeType\n\n ShapeType (*values)\n\nAvailable shape types in snake_case format\n\nsource\n\n\nReadinessResponse\n\n ReadinessResponse (success:bool, status:Literal['ready','not_ready'],\n                    code:int, message:str)\n\nReadiness check response\n\nsource\n\n\nHealthResponse\n\n HealthResponse (success:bool=True,\n                 status:Literal['ok','degraded','error'], code:int=200,\n                 message:str)\n\nHealth check response\n\nsource\n\n\nErrorResponse\n\n ErrorResponse (detail:__main__.ErrorDetail)\n\nError response wrapper\n\nsource\n\n\nErrorDetail\n\n ErrorDetail (success:bool=False, status:str, code:int, message:str)\n\nStandard error response structure",
    "crumbs": [
      "Models"
    ]
  },
  {
    "objectID": "color_extractor.html",
    "href": "color_extractor.html",
    "title": "Color Extractor",
    "section": "",
    "text": "source\n\nattach_colors\n\n attach_colors (image_bgr, detections)\n\n\nsource\n\n\nfind_dominant_color\n\n find_dominant_color (image, shape_type='rectangle', bbox=None, k=4,\n                      least_prominent=False, bg_rgb=(255, 255, 255),\n                      bg_thresh=24)\n\n\nsource\n\n\nextract_shape_mask\n\n extract_shape_mask (H, W, shape_type, bbox, vector_data=None)\n\n\nsource\n\n\nget_corner_background_color\n\n get_corner_background_color (image)\n\n\nsource\n\n\nrgb_to_hex\n\n rgb_to_hex (rgb_tuple)\n\n\nsource\n\n\nfind_dominant_color_masked\n\n find_dominant_color_masked (image_crop, mask=None, k=3)",
    "crumbs": [
      "Color Extractor"
    ]
  },
  {
    "objectID": "sam2_processor.html",
    "href": "sam2_processor.html",
    "title": "SAM2 Processor",
    "section": "",
    "text": "source\n\npredict_angles_with_masks\n\n predict_angles_with_masks (image_bgr, detections)",
    "crumbs": [
      "SAM2 Processor"
    ]
  },
  {
    "objectID": "diamond_handler.html",
    "href": "diamond_handler.html",
    "title": "Diamond Handler",
    "section": "",
    "text": "source\n\nfind_skeleton_points_on_bbox\n\n find_skeleton_points_on_bbox (skeleton, bbox_tl, bbox_br, margin=2)\n\n\nsource\n\n\ndilate_ocr_regions\n\n dilate_ocr_regions (binary_mask, ocr_intrusions, crop_origin,\n                     vert_expand=1, horiz_expand=1, vert_iterations=1,\n                     horiz_iterations=1, erosion_kernel_size=3,\n                     erosion_iterations=1)",
    "crumbs": [
      "Diamond Handler"
    ]
  },
  {
    "objectID": "text_mapper.html",
    "href": "text_mapper.html",
    "title": "Text Mapper",
    "section": "",
    "text": "source\n\nmap_ocr_to_nodes\n\n map_ocr_to_nodes (diagram_nodes, processed_ocr_data,\n                   relaxation_pixels=15, max_area_ratio=80)\n\n\nsource\n\n\nget_text_rotation_for_shape\n\n get_text_rotation_for_shape (shape_bbox, ocr_data, threshold=0.25)\n\n*Find OCR text within a shape and calculate weighted rotation angle.\nArgs: shape_bbox: (x1, y1, x2, y2) of the shape ocr_data: List of OCR data with polygon_rotation/shape_rotation threshold: Minimum overlap threshold (0-1)\nReturns: Snapped rotation angle (float) or None if no text found*",
    "crumbs": [
      "Text Mapper"
    ]
  },
  {
    "objectID": "edge_builder.html",
    "href": "edge_builder.html",
    "title": "Edge Builder",
    "section": "",
    "text": "source\n\ncreate_edges_json\n\n create_edges_json (arrow_connections, diagram_nodes, all_dets_with_color,\n                    arrowhead_radius=50)",
    "crumbs": [
      "Edge Builder"
    ]
  },
  {
    "objectID": "routers_spatial.html",
    "href": "routers_spatial.html",
    "title": "Spatial Router",
    "section": "",
    "text": "source\n\nspatial_map\n\n spatial_map\n              (file:fastapi.datastructures.UploadFile=File(PydanticUndefin\n              ed))\n\nConvert a diagram image to JSON spatial representation.\n\nsource\n\n\nhealth_check\n\n health_check ()\n\nSimple health check endpoint.\n\nsource\n\n\nreadiness_check\n\n readiness_check ()\n\nCheck if the service is ready to handle requests.\n\nsource\n\n\nSpatialMapResponse\n\n SpatialMapResponse (success:bool=True, status:str='ok', code:int=200,\n                     data:Dict[str,Any])\n\n*!!! abstract “Usage Documentation” Models\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of the class variables defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The synthesized __init__ [Signature][inspect.Signature] of the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The core schema of the model.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a [`RootModel`][pydantic.root_model.RootModel].\n__pydantic_serializer__: The `pydantic-core` `SchemaSerializer` used to dump instances of the model.\n__pydantic_validator__: The `pydantic-core` `SchemaValidator` used to validate instances of the model.\n\n__pydantic_fields__: A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n__pydantic_computed_fields__: A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n\n__pydantic_extra__: A dictionary containing extra values, if [`extra`][pydantic.config.ConfigDict.extra]\n    is set to `'allow'`.\n__pydantic_fields_set__: The names of fields explicitly set during instantiation.\n__pydantic_private__: Values of private attributes set on the model instance.*\n\nsource\n\n\nReadinessResponse\n\n ReadinessResponse (success:bool=True, status:str='ready', code:int=200,\n                    message:str='Service is ready to handle requests')\n\n*!!! abstract “Usage Documentation” Models\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of the class variables defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The synthesized __init__ [Signature][inspect.Signature] of the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The core schema of the model.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a [`RootModel`][pydantic.root_model.RootModel].\n__pydantic_serializer__: The `pydantic-core` `SchemaSerializer` used to dump instances of the model.\n__pydantic_validator__: The `pydantic-core` `SchemaValidator` used to validate instances of the model.\n\n__pydantic_fields__: A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n__pydantic_computed_fields__: A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n\n__pydantic_extra__: A dictionary containing extra values, if [`extra`][pydantic.config.ConfigDict.extra]\n    is set to `'allow'`.\n__pydantic_fields_set__: The names of fields explicitly set during instantiation.\n__pydantic_private__: Values of private attributes set on the model instance.*\n\nsource\n\n\nErrorDetail\n\n ErrorDetail (success:bool=False, status:str='error', code:int,\n              message:str)\n\n*!!! abstract “Usage Documentation” Models\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of the class variables defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The synthesized __init__ [Signature][inspect.Signature] of the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The core schema of the model.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a [`RootModel`][pydantic.root_model.RootModel].\n__pydantic_serializer__: The `pydantic-core` `SchemaSerializer` used to dump instances of the model.\n__pydantic_validator__: The `pydantic-core` `SchemaValidator` used to validate instances of the model.\n\n__pydantic_fields__: A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n__pydantic_computed_fields__: A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n\n__pydantic_extra__: A dictionary containing extra values, if [`extra`][pydantic.config.ConfigDict.extra]\n    is set to `'allow'`.\n__pydantic_fields_set__: The names of fields explicitly set during instantiation.\n__pydantic_private__: Values of private attributes set on the model instance.*",
    "crumbs": [
      "Spatial Router"
    ]
  },
  {
    "objectID": "download_models.html",
    "href": "download_models.html",
    "title": "Download Models",
    "section": "",
    "text": "source\n\ndownload_paddleocr_models\n\n download_paddleocr_models (lang:str='en')",
    "crumbs": [
      "Download Models"
    ]
  }
]